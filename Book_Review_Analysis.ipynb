{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "collapsed_sections": [
        "za4BCdudZhUo"
      ],
      "authorship_tag": "ABX9TyPXfJ+/xZqZAvn49ZkT2BEK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "61fa4ff17a634b9480fcf6a0dff289fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_549197c69f4a4b70b5fa98aa26299e5a",
              "IPY_MODEL_8524c25c86b14c3e9ddf53d6df5be42b",
              "IPY_MODEL_e40ab8b365054b6e92a5f4769362697d"
            ],
            "layout": "IPY_MODEL_f1e84a38110640b3a8987c65b09d4c99"
          }
        },
        "549197c69f4a4b70b5fa98aa26299e5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ec85ba773604e4aaa747a8f15135e8a",
            "placeholder": "​",
            "style": "IPY_MODEL_377d684a54794f9aac56e2c4ba142b02",
            "value": "100%"
          }
        },
        "8524c25c86b14c3e9ddf53d6df5be42b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7d390c2a82e4cb0bc5aaa58000f3762",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b01fa9d7b5ac465dbddc656938079eb4",
            "value": 100
          }
        },
        "e40ab8b365054b6e92a5f4769362697d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_105e877e5dbd402a932a60cf3a0df8aa",
            "placeholder": "​",
            "style": "IPY_MODEL_3e3ffa266fe741309d0644bcea8334dc",
            "value": " 100/100 [01:11&lt;00:00,  1.33it/s]"
          }
        },
        "f1e84a38110640b3a8987c65b09d4c99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ec85ba773604e4aaa747a8f15135e8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "377d684a54794f9aac56e2c4ba142b02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e7d390c2a82e4cb0bc5aaa58000f3762": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b01fa9d7b5ac465dbddc656938079eb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "105e877e5dbd402a932a60cf3a0df8aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e3ffa266fe741309d0644bcea8334dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f2aeddb3fa3a4410b66e7f7769384b66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_20fb77f5549e49039ee58ad38e2d8e1a",
              "IPY_MODEL_608c9fc55ae548b1852c41b125b6f7ee",
              "IPY_MODEL_c41aa8bfdf4e4053bd6f7782dd177700"
            ],
            "layout": "IPY_MODEL_b145860238484c9db407c41f3561159b"
          }
        },
        "20fb77f5549e49039ee58ad38e2d8e1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d16d07a74e4c489391e363856dbf7372",
            "placeholder": "​",
            "style": "IPY_MODEL_681e6701763e4a65964bfc90f241082a",
            "value": "100%"
          }
        },
        "608c9fc55ae548b1852c41b125b6f7ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe11730f79194bb1b6627ef9a6b0bf83",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_85abf3242d9348368081456b575acb58",
            "value": 100
          }
        },
        "c41aa8bfdf4e4053bd6f7782dd177700": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c84e8a108a2e4a05929e0299d9425300",
            "placeholder": "​",
            "style": "IPY_MODEL_efc829574cec419bacfc7dd8c4333451",
            "value": " 100/100 [32:19&lt;00:00, 18.70s/it]"
          }
        },
        "b145860238484c9db407c41f3561159b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d16d07a74e4c489391e363856dbf7372": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "681e6701763e4a65964bfc90f241082a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fe11730f79194bb1b6627ef9a6b0bf83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85abf3242d9348368081456b575acb58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c84e8a108a2e4a05929e0299d9425300": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "efc829574cec419bacfc7dd8c4333451": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/irishakarpova/BlockChainPuller/blob/main/Book_Review_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<font size=10>Data Science and Business Analytics</font>\n",
        "\n"
      ],
      "metadata": {
        "id": "U2VC_ZVQnfDG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Book Review Analysis"
      ],
      "metadata": {
        "id": "YCRLQ4s-nXhG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem Statement\n"
      ],
      "metadata": {
        "id": "AE8-KrwBZfDV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Business Context"
      ],
      "metadata": {
        "id": "za4BCdudZhUo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the book industry, keeping readers satisfied is essential for the success of individual books and the reputation of publishers. A global book platform wants to better understand and improve how readers feel about the books it offers. The company knows that reader reviews are a valuable way to learn about the quality of the writing, the strength of the story, and how much readers enjoy the book overall."
      ],
      "metadata": {
        "id": "_NhER0I5Z5hC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem Definition"
      ],
      "metadata": {
        "id": "ftZrhyLGaDiW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even though there are plenty of reader reviews available, the company struggles to turn them into useful insights. Reading through large amounts of text by hand takes a lot of time and can’t be done efficiently at scale. The main challenges include:\n",
        "\n",
        "**Unstructured Data:** Reader reviews are written in casual written language, which makes it hard to quickly find important information about the book’s content, writing style, and reader reactions.\n",
        "\n",
        "**Large Volume of Reviews**: With thousands of books in many different genres, the platform collects a huge number of reviews. Going through all of them manually is not practical, so an automated approach is needed.\n",
        "\n",
        "**Understanding Reader Sentiment:** Figuring out whether a review is positive, negative, or neutral can be difficult. But this is important for learning what readers like or don`t like, improving book suggestions, and helping with marketing and publishing decisions."
      ],
      "metadata": {
        "id": "b3qjH2_xaaYi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Objective"
      ],
      "metadata": {
        "id": "KvhDvpz1afu1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The objective is to develop a sentiment analysis model leveraging a Large Language Model (LLM) to accurately classify reader sentiments—positive, negative, or neutral from unstructured book reviews. This model aims to enable scalable analysis of reader feedback, provide deeper insights into audience preferences, support data-driven editorial and marketing decisions, and ultimately enhance overall reader satisfaction on the platform."
      ],
      "metadata": {
        "id": "z7AM1Nc0aqKv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing and Importing Libraries"
      ],
      "metadata": {
        "id": "ytzzr2FOntjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installation for GPU llama-cpp-python\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.2.45 --force-reinstall --no-cache-dir -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3WyoXoRvmRy9",
        "outputId": "735d6b21-dfb8-4632-d177-09e7fec8b74d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.7/36.7 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m201.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m290.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m382.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m324.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m308.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "cupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.3.1 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.3.1 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub==0.33.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "lFNUzbJjlJJo",
        "outputId": "16f1360d-e92d-433a-849c-a3004759528c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting huggingface_hub==0.33.1\n",
            "  Downloading huggingface_hub-0.33.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.33.1) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.33.1) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.33.1) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.33.1) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.33.1) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.33.1) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.33.1) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.33.1) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub==0.33.1) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub==0.33.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub==0.33.1) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub==0.33.1) (2025.6.15)\n",
            "Downloading huggingface_hub-0.33.1-py3-none-any.whl (515 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/515.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m512.0/515.4 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m515.4/515.4 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: huggingface_hub\n",
            "  Attempting uninstall: huggingface_hub\n",
            "    Found existing installation: huggingface-hub 0.33.0\n",
            "    Uninstalling huggingface-hub-0.33.0:\n",
            "      Successfully uninstalled huggingface-hub-0.33.0\n",
            "Successfully installed huggingface_hub-0.33.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y numpy\n",
        "!pip uninstall -y scikit-learn\n",
        "!pip install numpy --upgrade --no-cache-dir\n",
        "!pip install scikit-learn --upgrade --no-cache-dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 798
        },
        "id": "j8b6620e2zfF",
        "outputId": "4ed2eeff-de2f-4455-ecbd-da82473be0ad"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 2.3.1\n",
            "Uninstalling numpy-2.3.1:\n",
            "  Successfully uninstalled numpy-2.3.1\n",
            "\u001b[33mWARNING: Skipping scikit-learn as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting numpy\n",
            "  Downloading numpy-2.3.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.3.1-cp311-cp311-manylinux_2_28_x86_64.whl (16.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m310.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "libpysal 4.13.0 requires scikit-learn>=1.1, which is not installed.\n",
            "librosa 0.11.0 requires scikit-learn>=1.1.0, which is not installed.\n",
            "hdbscan 0.8.40 requires scikit-learn>=0.20, which is not installed.\n",
            "umap-learn 0.5.7 requires scikit-learn>=0.22, which is not installed.\n",
            "mlxtend 0.23.4 requires scikit-learn>=1.3.1, which is not installed.\n",
            "tsfresh 0.21.0 requires scikit-learn>=0.22.0, which is not installed.\n",
            "shap 0.48.0 requires scikit-learn, which is not installed.\n",
            "sklearn-pandas 2.2.0 requires scikit-learn>=0.23.0, which is not installed.\n",
            "fastai 2.7.19 requires scikit-learn, which is not installed.\n",
            "imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, which is not installed.\n",
            "pynndescent 0.5.13 requires scikit-learn>=0.18, which is not installed.\n",
            "yellowbrick 1.5 requires scikit-learn>=1.0.0, which is not installed.\n",
            "sentence-transformers 4.1.0 requires scikit-learn, which is not installed.\n",
            "cupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.3.1 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.3.1 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.3.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "9efa0f814c3f429790738066a78760e2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (17 kB)\n",
            "Requirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.3.1)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Downloading scikit_learn-1.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m309.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2b53ea8",
        "outputId": "55f226a2-d2d6-43b2-c2e1-0fa2bd93b3c9"
      },
      "source": [
        "!pip install scikit-learn"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (17 kB)\n",
            "Requirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.3.1)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Downloading scikit_learn-1.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.9 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/12.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/12.9 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/12.9 MB\u001b[0m \u001b[31m110.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m213.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m118.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scikit-learn\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed scikit-learn-1.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing library for data manipulation\n",
        "import pandas as pd\n",
        "\n",
        "# Function to download the model from the Hugging Face model hub\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# Importing the Llama class from the llama_cpp module\n",
        "from llama_cpp import Llama\n",
        "\n",
        "# Importing the json module\n",
        "import json\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Compare true vs predicted\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
      ],
      "metadata": {
        "id": "OzB91jOilSv9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import the dataset"
      ],
      "metadata": {
        "id": "Itg8C7IvnoyD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlsNiKGjnoea",
        "outputId": "c6b107a0-095b-4d27-99de-b9279010bf14"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/Projects/Generative AI/g_reviews.csv\")"
      ],
      "metadata": {
        "id": "Hx0IBAvbn5U5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Overview"
      ],
      "metadata": {
        "id": "wEXNvxfuoA8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "qnQ_CddvoIrD",
        "outputId": "ec744d33-c717-4ef4-9a65-ff6cd3c090d4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                          ReviewText  Sentiment\n",
              "0              Review #1: Highly disappointing read.          0\n",
              "1  Review #2: A page-turner with a powerful message.          2\n",
              "2          Review #3: A masterpiece of storytelling.          2\n",
              "3        Review #4: Heartwarming and inspiring read.          2\n",
              "4        Review #5: Neither good nor bad, just fine.          1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ed15373c-cf29-4876-8545-9c7301235239\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ReviewText</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Review #1: Highly disappointing read.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Review #2: A page-turner with a powerful message.</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Review #3: A masterpiece of storytelling.</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Review #4: Heartwarming and inspiring read.</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Review #5: Neither good nor bad, just fine.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ed15373c-cf29-4876-8545-9c7301235239')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ed15373c-cf29-4876-8545-9c7301235239 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ed15373c-cf29-4876-8545-9c7301235239');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-3a046036-2727-4378-a561-78b8bcfd0b75\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3a046036-2727-4378-a561-78b8bcfd0b75')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-3a046036-2727-4378-a561-78b8bcfd0b75 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 1209,\n  \"fields\": [\n    {\n      \"column\": \"ReviewText\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1209,\n        \"samples\": [\n          \"Review #102: Uninspiring and boring.\",\n          \"Review #433: Struggled to stay interested.\",\n          \"Review #310: Typical story, met expectations.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sentiment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0,\n          2,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = df.copy()"
      ],
      "metadata": {
        "id": "TMJN0zxxRCow"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dJmoO4_EKaZ",
        "outputId": "e320fb4b-a322-4068-9811-cbd509c5c9d2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1209, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations**\n",
        "\n",
        "- Data has 1209 rows and 2 columns"
      ],
      "metadata": {
        "id": "FdNTpI3toUc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# checking for missing values\n",
        "data.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "M8U37Tjiofzp",
        "outputId": "9b00cdd3-5090-4364-fc4c-e6ae56794775"
      },
      "execution_count": 282,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ReviewText    0\n",
              "Sentiment     0\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>ReviewText</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Sentiment</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 282
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations**\n",
        "\n",
        "- There are no missing values in the data"
      ],
      "metadata": {
        "id": "VGc5Vau5ojPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of reviews per sentiment\n",
        "sentiment_counts = data['Sentiment'].value_counts().sort_index()\n",
        "\n",
        "# Map sentiment labels for readability\n",
        "sentiment_labels = {0: 'Negative', 1: 'Neutral', 2: 'Positive'}\n",
        "sentiment_counts.index = sentiment_counts.index.map(sentiment_labels)"
      ],
      "metadata": {
        "id": "7PSQlvldpWOV"
      },
      "execution_count": 283,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation:**\n",
        "\n",
        "1. The dataset is fairly balanced across all three sentiment categories:\n",
        "\n",
        "- Positive: 33.5%\n",
        "- Neutral: 34.2%\n",
        "- Negative: 32.3%\n",
        "\n",
        "2. It indicates that the dataset includes a good variety of reader opinions, which can help build a more generalizable model."
      ],
      "metadata": {
        "id": "tp6-ecSOpoDy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utility function"
      ],
      "metadata": {
        "id": "NqzMDWfgD_hS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# defining a function to parse the JSON output from the model\n",
        "def extract_json_data(json_str):\n",
        "    try:\n",
        "        # Find the indices of the opening and closing curly braces\n",
        "        json_start = json_str.find('{')\n",
        "        json_end = json_str.rfind('}')\n",
        "\n",
        "        if json_start != -1 and json_end != -1:\n",
        "            extracted_sentiment = json_str[json_start:json_end + 1]  # Extract the JSON object\n",
        "            data_dict = json.loads(extracted_sentiment)\n",
        "            return data_dict\n",
        "        else:\n",
        "            print(f\"Warning: JSON object not found in response: {json_str}\")\n",
        "            return {}\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Error parsing JSON: {e}\")\n",
        "        return {}"
      ],
      "metadata": {
        "id": "d3wZUybhD9ZD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Llama Model Building"
      ],
      "metadata": {
        "id": "HFTXXigKn8dj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This is Hugging Face model repository created by TheBloke, who publishes quantized GGUF-format models for use with llama.cpp\n",
        "model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGUF\"\n",
        "model_basename = \"llama-2-13b-chat.Q5_K_M.gguf\""
      ],
      "metadata": {
        "id": "OCa5hssPn_Da"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The filename parameter specifies the name of the file to download\n",
        "model_path = hf_hub_download(\n",
        "    repo_id=model_name_or_path,\n",
        "    filename=model_basename\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-eVyGeNHpbzK",
        "outputId": "eee90895-eb83-4640-f50f-974fc4117ec5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klSRQHqsvk6x",
        "outputId": "d90333c1-2ed8-4264-f8e4-f0e378df39c1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['ReviewText', 'Sentiment'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# I use T4 GPU GPU RAM 15.0 GB that is why n_gpu_layers=40\n",
        "\n",
        "llm = Llama(\n",
        "    model_path=model_path,\n",
        "    n_ctx=4096,\n",
        "    n_threads=8,\n",
        "    n_gpu_layers=40,\n",
        "    f16_kv=True,\n",
        "    use_mmap=True,\n",
        "    use_mlock=False\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UsucVv7pMGb",
        "outputId": "aa03214f-276b-4938-ee22-1e7c0bc380a3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 19 key-value pairs and 363 tensors from /root/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGUF/snapshots/4458acc949de0a9914c3eab623904d4fe999050a/llama-2-13b-chat.Q5_K_M.gguf (version GGUF V2)\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 17\n",
            "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   81 tensors\n",
            "llama_model_loader: - type q5_K:  241 tensors\n",
            "llama_model_loader: - type q6_K:   41 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
            "llm_load_print_meta: format           = GGUF V2\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 4096\n",
            "llm_load_print_meta: n_embd           = 5120\n",
            "llm_load_print_meta: n_head           = 40\n",
            "llm_load_print_meta: n_head_kv        = 40\n",
            "llm_load_print_meta: n_layer          = 40\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 5120\n",
            "llm_load_print_meta: n_embd_v_gqa     = 5120\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 13824\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: model type       = 13B\n",
            "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
            "llm_load_print_meta: model params     = 13.02 B\n",
            "llm_load_print_meta: model size       = 8.60 GiB (5.67 BPW) \n",
            "llm_load_print_meta: general.name     = LLaMA v2\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.28 MiB\n",
            "llm_load_tensors: offloading 40 repeating layers to GPU\n",
            "llm_load_tensors: offloaded 40/41 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =  8801.63 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size =  8566.02 MiB\n",
            "....................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 4096\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =  3200.00 MiB\n",
            "llama_new_context_with_model: KV self size  = 3200.00 MiB, K (f16): 1600.00 MiB, V (f16): 1600.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host input buffer size   =    19.04 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =   368.02 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =    82.50 MiB\n",
            "llama_new_context_with_model: graph splits (measure): 4\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
            "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '5120', 'llama.feed_forward_length': '13824', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '40', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '40', 'llama.attention.head_count_kv': '40', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '17'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Response Parameters"
      ],
      "metadata": {
        "id": "z4rE0TFCwqoM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_llama_response(instruction, review):\n",
        "    prompt = f\"\"\"[INST] <<SYS>>\n",
        "{instruction.strip()}\n",
        "<</SYS>>\n",
        "\n",
        "{review.strip()}\n",
        "[/INST]\"\"\"\n",
        "\n",
        "    response = llm(\n",
        "        prompt=prompt,\n",
        "        max_tokens=512,\n",
        "        temperature=0,\n",
        "        top_p=1.0,\n",
        "        top_k=0,\n",
        "        repeat_penalty=1.0,\n",
        "        echo=False,\n",
        "        seed=42,\n",
        "    )\n",
        "\n",
        "    return response[\"choices\"][0][\"text\"].strip()"
      ],
      "metadata": {
        "id": "ClVpI6hHwvb0"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Sentiment Analysis (Llama)"
      ],
      "metadata": {
        "id": "oxi_DMsyxBjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a copy of the data\n",
        "# data_1 = data.copy()\n",
        "data_1 = data.iloc[:100].copy()"
      ],
      "metadata": {
        "id": "V8ITsUKvxH4I"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# defining the instructions for the model\n",
        "\n",
        "instruction = \"\"\"\n",
        "You are a sentiment classifier.\n",
        "\n",
        "Read the following book review and respond with exactly one word:\n",
        "Positive, Neutral, or Negative.\n",
        "\n",
        "Do not include any greetings, explanations, or extra text.\n",
        "Only respond with one of the three words — exactly as written.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Fe9XqMQdxKD6"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import tqdm for progress tracking in notebooks\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Enable progress bar for pandas apply functions\n",
        "tqdm.pandas()\n",
        "\n",
        "# Apply the LLaMA model to each review and generate a sentiment prediction\n",
        "data_1['model_response'] = data_1['ReviewText'].progress_apply(\n",
        "    lambda x: generate_llama_response(instruction, x)\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "61fa4ff17a634b9480fcf6a0dff289fb",
            "549197c69f4a4b70b5fa98aa26299e5a",
            "8524c25c86b14c3e9ddf53d6df5be42b",
            "e40ab8b365054b6e92a5f4769362697d",
            "f1e84a38110640b3a8987c65b09d4c99",
            "6ec85ba773604e4aaa747a8f15135e8a",
            "377d684a54794f9aac56e2c4ba142b02",
            "e7d390c2a82e4cb0bc5aaa58000f3762",
            "b01fa9d7b5ac465dbddc656938079eb4",
            "105e877e5dbd402a932a60cf3a0df8aa",
            "3e3ffa266fe741309d0644bcea8334dc"
          ]
        },
        "id": "BLW-o_nDDD_g",
        "outputId": "728281de-0522-409a-e795-f1585bb3a96d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "61fa4ff17a634b9480fcf6a0dff289fb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.17 ms /     4 runs   (    0.04 ms per token, 23809.52 tokens per second)\n",
            "llama_print_timings: prompt eval time =     518.77 ms /    97 tokens (    5.35 ms per token,   186.98 tokens per second)\n",
            "llama_print_timings:        eval time =     154.48 ms /     3 runs   (   51.49 ms per token,    19.42 tokens per second)\n",
            "llama_print_timings:       total time =     686.67 ms /   100 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.16 ms /     4 runs   (    0.04 ms per token, 24390.24 tokens per second)\n",
            "llama_print_timings: prompt eval time =     495.57 ms /    17 tokens (   29.15 ms per token,    34.30 tokens per second)\n",
            "llama_print_timings:        eval time =     154.25 ms /     3 runs   (   51.42 ms per token,    19.45 tokens per second)\n",
            "llama_print_timings:       total time =     659.78 ms /    20 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.16 ms /     4 runs   (    0.04 ms per token, 24691.36 tokens per second)\n",
            "llama_print_timings: prompt eval time =     398.74 ms /    16 tokens (   24.92 ms per token,    40.13 tokens per second)\n",
            "llama_print_timings:        eval time =     156.35 ms /     3 runs   (   52.12 ms per token,    19.19 tokens per second)\n",
            "llama_print_timings:       total time =     565.43 ms /    19 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.16 ms /     4 runs   (    0.04 ms per token, 24691.36 tokens per second)\n",
            "llama_print_timings: prompt eval time =     406.01 ms /    16 tokens (   25.38 ms per token,    39.41 tokens per second)\n",
            "llama_print_timings:        eval time =     159.56 ms /     3 runs   (   53.19 ms per token,    18.80 tokens per second)\n",
            "llama_print_timings:       total time =     575.49 ms /    19 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.20 ms /     5 runs   (    0.04 ms per token, 24630.54 tokens per second)\n",
            "llama_print_timings: prompt eval time =     405.05 ms /    16 tokens (   25.32 ms per token,    39.50 tokens per second)\n",
            "llama_print_timings:        eval time =     228.08 ms /     4 runs   (   57.02 ms per token,    17.54 tokens per second)\n",
            "llama_print_timings:       total time =     645.06 ms /    20 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.16 ms /     4 runs   (    0.04 ms per token, 24691.36 tokens per second)\n",
            "llama_print_timings: prompt eval time =     400.32 ms /    14 tokens (   28.59 ms per token,    34.97 tokens per second)\n",
            "llama_print_timings:        eval time =     154.44 ms /     3 runs   (   51.48 ms per token,    19.43 tokens per second)\n",
            "llama_print_timings:       total time =     564.95 ms /    17 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.16 ms /     4 runs   (    0.04 ms per token, 24390.24 tokens per second)\n",
            "llama_print_timings: prompt eval time =     490.45 ms /    17 tokens (   28.85 ms per token,    34.66 tokens per second)\n",
            "llama_print_timings:        eval time =     157.32 ms /     3 runs   (   52.44 ms per token,    19.07 tokens per second)\n",
            "llama_print_timings:       total time =     657.77 ms /    20 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.22 ms /     5 runs   (    0.04 ms per token, 22727.27 tokens per second)\n",
            "llama_print_timings: prompt eval time =     401.23 ms /    15 tokens (   26.75 ms per token,    37.39 tokens per second)\n",
            "llama_print_timings:        eval time =     227.71 ms /     4 runs   (   56.93 ms per token,    17.57 tokens per second)\n",
            "llama_print_timings:       total time =     642.04 ms /    19 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.20 ms /     5 runs   (    0.04 ms per token, 24630.54 tokens per second)\n",
            "llama_print_timings: prompt eval time =     402.93 ms /    15 tokens (   26.86 ms per token,    37.23 tokens per second)\n",
            "llama_print_timings:        eval time =     229.31 ms /     4 runs   (   57.33 ms per token,    17.44 tokens per second)\n",
            "llama_print_timings:       total time =     645.04 ms /    19 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.22 ms /     5 runs   (    0.04 ms per token, 22831.05 tokens per second)\n",
            "llama_print_timings: prompt eval time =     498.29 ms /    18 tokens (   27.68 ms per token,    36.12 tokens per second)\n",
            "llama_print_timings:        eval time =     218.13 ms /     4 runs   (   54.53 ms per token,    18.34 tokens per second)\n",
            "llama_print_timings:       total time =     728.68 ms /    22 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.21 ms /     5 runs   (    0.04 ms per token, 24154.59 tokens per second)\n",
            "llama_print_timings: prompt eval time =     529.05 ms /    18 tokens (   29.39 ms per token,    34.02 tokens per second)\n",
            "llama_print_timings:        eval time =     373.38 ms /     4 runs   (   93.35 ms per token,    10.71 tokens per second)\n",
            "llama_print_timings:       total time =     915.83 ms /    22 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.16 ms /     4 runs   (    0.04 ms per token, 24844.72 tokens per second)\n",
            "llama_print_timings: prompt eval time =     416.54 ms /    14 tokens (   29.75 ms per token,    33.61 tokens per second)\n",
            "llama_print_timings:        eval time =     328.41 ms /     3 runs   (  109.47 ms per token,     9.13 tokens per second)\n",
            "llama_print_timings:       total time =     755.54 ms /    17 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.22 ms /     5 runs   (    0.04 ms per token, 22935.78 tokens per second)\n",
            "llama_print_timings: prompt eval time =     420.79 ms /    15 tokens (   28.05 ms per token,    35.65 tokens per second)\n",
            "llama_print_timings:        eval time =     277.62 ms /     4 runs   (   69.40 ms per token,    14.41 tokens per second)\n",
            "llama_print_timings:       total time =     711.04 ms /    19 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.24 ms /     6 runs   (    0.04 ms per token, 24590.16 tokens per second)\n",
            "llama_print_timings: prompt eval time =     405.65 ms /    16 tokens (   25.35 ms per token,    39.44 tokens per second)\n",
            "llama_print_timings:        eval time =     286.95 ms /     5 runs   (   57.39 ms per token,    17.42 tokens per second)\n",
            "llama_print_timings:       total time =     707.47 ms /    21 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.17 ms /     4 runs   (    0.04 ms per token, 23529.41 tokens per second)\n",
            "llama_print_timings: prompt eval time =     496.96 ms /    17 tokens (   29.23 ms per token,    34.21 tokens per second)\n",
            "llama_print_timings:        eval time =     177.85 ms /     3 runs   (   59.28 ms per token,    16.87 tokens per second)\n",
            "llama_print_timings:       total time =     684.38 ms /    20 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.20 ms /     5 runs   (    0.04 ms per token, 24752.48 tokens per second)\n",
            "llama_print_timings: prompt eval time =     499.53 ms /    18 tokens (   27.75 ms per token,    36.03 tokens per second)\n",
            "llama_print_timings:        eval time =     238.42 ms /     4 runs   (   59.60 ms per token,    16.78 tokens per second)\n",
            "llama_print_timings:       total time =     750.50 ms /    22 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.20 ms /     5 runs   (    0.04 ms per token, 24875.62 tokens per second)\n",
            "llama_print_timings: prompt eval time =     419.79 ms /    15 tokens (   27.99 ms per token,    35.73 tokens per second)\n",
            "llama_print_timings:        eval time =     223.40 ms /     4 runs   (   55.85 ms per token,    17.91 tokens per second)\n",
            "llama_print_timings:       total time =     656.44 ms /    19 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.23 ms /     5 runs   (    0.05 ms per token, 21551.72 tokens per second)\n",
            "llama_print_timings: prompt eval time =     408.92 ms /    16 tokens (   25.56 ms per token,    39.13 tokens per second)\n",
            "llama_print_timings:        eval time =     221.46 ms /     4 runs   (   55.36 ms per token,    18.06 tokens per second)\n",
            "llama_print_timings:       total time =     643.36 ms /    20 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.20 ms /     5 runs   (    0.04 ms per token, 24875.62 tokens per second)\n",
            "llama_print_timings: prompt eval time =     412.41 ms /    15 tokens (   27.49 ms per token,    36.37 tokens per second)\n",
            "llama_print_timings:        eval time =     238.05 ms /     4 runs   (   59.51 ms per token,    16.80 tokens per second)\n",
            "llama_print_timings:       total time =     662.32 ms /    19 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.16 ms /     4 runs   (    0.04 ms per token, 24691.36 tokens per second)\n",
            "llama_print_timings: prompt eval time =     408.84 ms /    14 tokens (   29.20 ms per token,    34.24 tokens per second)\n",
            "llama_print_timings:        eval time =     164.30 ms /     3 runs   (   54.77 ms per token,    18.26 tokens per second)\n",
            "llama_print_timings:       total time =     583.16 ms /    17 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.18 ms /     4 runs   (    0.05 ms per token, 21978.02 tokens per second)\n",
            "llama_print_timings: prompt eval time =     406.65 ms /    14 tokens (   29.05 ms per token,    34.43 tokens per second)\n",
            "llama_print_timings:        eval time =     164.14 ms /     3 runs   (   54.71 ms per token,    18.28 tokens per second)\n",
            "llama_print_timings:       total time =     580.50 ms /    17 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.21 ms /     5 runs   (    0.04 ms per token, 24390.24 tokens per second)\n",
            "llama_print_timings: prompt eval time =     503.07 ms /    17 tokens (   29.59 ms per token,    33.79 tokens per second)\n",
            "llama_print_timings:        eval time =     234.47 ms /     4 runs   (   58.62 ms per token,    17.06 tokens per second)\n",
            "llama_print_timings:       total time =     760.22 ms /    21 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.16 ms /     4 runs   (    0.04 ms per token, 24844.72 tokens per second)\n",
            "llama_print_timings: prompt eval time =     403.36 ms /    16 tokens (   25.21 ms per token,    39.67 tokens per second)\n",
            "llama_print_timings:        eval time =     176.06 ms /     3 runs   (   58.69 ms per token,    17.04 tokens per second)\n",
            "llama_print_timings:       total time =     588.46 ms /    19 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.22 ms /     5 runs   (    0.04 ms per token, 22522.52 tokens per second)\n",
            "llama_print_timings: prompt eval time =     507.45 ms /    17 tokens (   29.85 ms per token,    33.50 tokens per second)\n",
            "llama_print_timings:        eval time =     229.08 ms /     4 runs   (   57.27 ms per token,    17.46 tokens per second)\n",
            "llama_print_timings:       total time =     749.74 ms /    21 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.16 ms /     4 runs   (    0.04 ms per token, 24539.88 tokens per second)\n",
            "llama_print_timings: prompt eval time =     412.58 ms /    15 tokens (   27.51 ms per token,    36.36 tokens per second)\n",
            "llama_print_timings:        eval time =     172.47 ms /     3 runs   (   57.49 ms per token,    17.39 tokens per second)\n",
            "llama_print_timings:       total time =     594.26 ms /    18 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.16 ms /     4 runs   (    0.04 ms per token, 24390.24 tokens per second)\n",
            "llama_print_timings: prompt eval time =     507.61 ms /    17 tokens (   29.86 ms per token,    33.49 tokens per second)\n",
            "llama_print_timings:        eval time =     175.97 ms /     3 runs   (   58.66 ms per token,    17.05 tokens per second)\n",
            "llama_print_timings:       total time =     693.86 ms /    20 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.16 ms /     4 runs   (    0.04 ms per token, 24844.72 tokens per second)\n",
            "llama_print_timings: prompt eval time =     501.03 ms /    17 tokens (   29.47 ms per token,    33.93 tokens per second)\n",
            "llama_print_timings:        eval time =     163.97 ms /     3 runs   (   54.66 ms per token,    18.30 tokens per second)\n",
            "llama_print_timings:       total time =     674.72 ms /    20 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.16 ms /     4 runs   (    0.04 ms per token, 24390.24 tokens per second)\n",
            "llama_print_timings: prompt eval time =     419.55 ms /    15 tokens (   27.97 ms per token,    35.75 tokens per second)\n",
            "llama_print_timings:        eval time =     172.21 ms /     3 runs   (   57.40 ms per token,    17.42 tokens per second)\n",
            "llama_print_timings:       total time =     602.99 ms /    18 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.16 ms /     4 runs   (    0.04 ms per token, 24390.24 tokens per second)\n",
            "llama_print_timings: prompt eval time =     440.16 ms /    15 tokens (   29.34 ms per token,    34.08 tokens per second)\n",
            "llama_print_timings:        eval time =     175.15 ms /     3 runs   (   58.38 ms per token,    17.13 tokens per second)\n",
            "llama_print_timings:       total time =     626.11 ms /    18 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.20 ms /     5 runs   (    0.04 ms per token, 24752.48 tokens per second)\n",
            "llama_print_timings: prompt eval time =     441.66 ms /    16 tokens (   27.60 ms per token,    36.23 tokens per second)\n",
            "llama_print_timings:        eval time =     431.32 ms /     4 runs   (  107.83 ms per token,     9.27 tokens per second)\n",
            "llama_print_timings:       total time =     885.09 ms /    20 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.20 ms /     5 runs   (    0.04 ms per token, 24752.48 tokens per second)\n",
            "llama_print_timings: prompt eval time =     496.41 ms /    18 tokens (   27.58 ms per token,    36.26 tokens per second)\n",
            "llama_print_timings:        eval time =     230.37 ms /     4 runs   (   57.59 ms per token,    17.36 tokens per second)\n",
            "llama_print_timings:       total time =     739.23 ms /    22 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.25 ms /     6 runs   (    0.04 ms per token, 23904.38 tokens per second)\n",
            "llama_print_timings: prompt eval time =     427.18 ms /    16 tokens (   26.70 ms per token,    37.45 tokens per second)\n",
            "llama_print_timings:        eval time =     297.13 ms /     5 runs   (   59.43 ms per token,    16.83 tokens per second)\n",
            "llama_print_timings:       total time =     738.69 ms /    21 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.20 ms /     5 runs   (    0.04 ms per token, 24509.80 tokens per second)\n",
            "llama_print_timings: prompt eval time =     420.12 ms /    15 tokens (   28.01 ms per token,    35.70 tokens per second)\n",
            "llama_print_timings:        eval time =     249.76 ms /     4 runs   (   62.44 ms per token,    16.02 tokens per second)\n",
            "llama_print_timings:       total time =     682.68 ms /    19 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.27 ms /     6 runs   (    0.04 ms per token, 22471.91 tokens per second)\n",
            "llama_print_timings: prompt eval time =     440.49 ms /    16 tokens (   27.53 ms per token,    36.32 tokens per second)\n",
            "llama_print_timings:        eval time =     306.65 ms /     5 runs   (   61.33 ms per token,    16.31 tokens per second)\n",
            "llama_print_timings:       total time =     762.40 ms /    21 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.16 ms /     4 runs   (    0.04 ms per token, 24691.36 tokens per second)\n",
            "llama_print_timings: prompt eval time =     419.71 ms /    14 tokens (   29.98 ms per token,    33.36 tokens per second)\n",
            "llama_print_timings:        eval time =     186.71 ms /     3 runs   (   62.24 ms per token,    16.07 tokens per second)\n",
            "llama_print_timings:       total time =     616.45 ms /    17 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.16 ms /     4 runs   (    0.04 ms per token, 24691.36 tokens per second)\n",
            "llama_print_timings: prompt eval time =     513.01 ms /    17 tokens (   30.18 ms per token,    33.14 tokens per second)\n",
            "llama_print_timings:        eval time =     170.38 ms /     3 runs   (   56.79 ms per token,    17.61 tokens per second)\n",
            "llama_print_timings:       total time =     693.42 ms /    20 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.16 ms /     4 runs   (    0.04 ms per token, 24390.24 tokens per second)\n",
            "llama_print_timings: prompt eval time =     429.43 ms /    15 tokens (   28.63 ms per token,    34.93 tokens per second)\n",
            "llama_print_timings:        eval time =     178.33 ms /     3 runs   (   59.44 ms per token,    16.82 tokens per second)\n",
            "llama_print_timings:       total time =     617.35 ms /    18 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.17 ms /     4 runs   (    0.04 ms per token, 23668.64 tokens per second)\n",
            "llama_print_timings: prompt eval time =     424.30 ms /    14 tokens (   30.31 ms per token,    33.00 tokens per second)\n",
            "llama_print_timings:        eval time =     178.62 ms /     3 runs   (   59.54 ms per token,    16.80 tokens per second)\n",
            "llama_print_timings:       total time =     612.56 ms /    17 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.21 ms /     5 runs   (    0.04 ms per token, 24154.59 tokens per second)\n",
            "llama_print_timings: prompt eval time =     542.52 ms /    17 tokens (   31.91 ms per token,    31.34 tokens per second)\n",
            "llama_print_timings:        eval time =     241.01 ms /     4 runs   (   60.25 ms per token,    16.60 tokens per second)\n",
            "llama_print_timings:       total time =     795.57 ms /    21 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.16 ms /     4 runs   (    0.04 ms per token, 24390.24 tokens per second)\n",
            "llama_print_timings: prompt eval time =     517.48 ms /    17 tokens (   30.44 ms per token,    32.85 tokens per second)\n",
            "llama_print_timings:        eval time =     197.81 ms /     3 runs   (   65.94 ms per token,    15.17 tokens per second)\n",
            "llama_print_timings:       total time =     725.34 ms /    20 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.16 ms /     4 runs   (    0.04 ms per token, 24390.24 tokens per second)\n",
            "llama_print_timings: prompt eval time =     441.68 ms /    16 tokens (   27.60 ms per token,    36.23 tokens per second)\n",
            "llama_print_timings:        eval time =     179.83 ms /     3 runs   (   59.94 ms per token,    16.68 tokens per second)\n",
            "llama_print_timings:       total time =     631.19 ms /    19 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.26 ms /     6 runs   (    0.04 ms per token, 22900.76 tokens per second)\n",
            "llama_print_timings: prompt eval time =     440.68 ms /    16 tokens (   27.54 ms per token,    36.31 tokens per second)\n",
            "llama_print_timings:        eval time =     332.82 ms /     5 runs   (   66.56 ms per token,    15.02 tokens per second)\n",
            "llama_print_timings:       total time =     789.14 ms /    21 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.18 ms /     4 runs   (    0.04 ms per token, 22727.27 tokens per second)\n",
            "llama_print_timings: prompt eval time =     428.25 ms /    16 tokens (   26.77 ms per token,    37.36 tokens per second)\n",
            "llama_print_timings:        eval time =     190.33 ms /     3 runs   (   63.44 ms per token,    15.76 tokens per second)\n",
            "llama_print_timings:       total time =     628.08 ms /    19 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.18 ms /     4 runs   (    0.05 ms per token, 21857.92 tokens per second)\n",
            "llama_print_timings: prompt eval time =     323.33 ms /    12 tokens (   26.94 ms per token,    37.11 tokens per second)\n",
            "llama_print_timings:        eval time =     175.63 ms /     3 runs   (   58.54 ms per token,    17.08 tokens per second)\n",
            "llama_print_timings:       total time =     509.98 ms /    15 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.18 ms /     4 runs   (    0.05 ms per token, 22099.45 tokens per second)\n",
            "llama_print_timings: prompt eval time =     434.25 ms /    16 tokens (   27.14 ms per token,    36.85 tokens per second)\n",
            "llama_print_timings:        eval time =     180.70 ms /     3 runs   (   60.23 ms per token,    16.60 tokens per second)\n",
            "llama_print_timings:       total time =     625.29 ms /    19 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.21 ms /     5 runs   (    0.04 ms per token, 23474.18 tokens per second)\n",
            "llama_print_timings: prompt eval time =     578.46 ms /    18 tokens (   32.14 ms per token,    31.12 tokens per second)\n",
            "llama_print_timings:        eval time =     353.26 ms /     4 runs   (   88.32 ms per token,    11.32 tokens per second)\n",
            "llama_print_timings:       total time =     945.01 ms /    22 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.24 ms /     6 runs   (    0.04 ms per token, 24489.80 tokens per second)\n",
            "llama_print_timings: prompt eval time =     454.67 ms /    16 tokens (   28.42 ms per token,    35.19 tokens per second)\n",
            "llama_print_timings:        eval time =     562.76 ms /     5 runs   (  112.55 ms per token,     8.88 tokens per second)\n",
            "llama_print_timings:       total time =    1033.54 ms /    21 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.17 ms /     4 runs   (    0.04 ms per token, 24096.39 tokens per second)\n",
            "llama_print_timings: prompt eval time =     513.53 ms /    17 tokens (   30.21 ms per token,    33.10 tokens per second)\n",
            "llama_print_timings:        eval time =     171.91 ms /     3 runs   (   57.30 ms per token,    17.45 tokens per second)\n",
            "llama_print_timings:       total time =     695.02 ms /    20 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.22 ms /     5 runs   (    0.04 ms per token, 22522.52 tokens per second)\n",
            "llama_print_timings: prompt eval time =     453.25 ms /    15 tokens (   30.22 ms per token,    33.09 tokens per second)\n",
            "llama_print_timings:        eval time =     249.04 ms /     4 runs   (   62.26 ms per token,    16.06 tokens per second)\n",
            "llama_print_timings:       total time =     714.98 ms /    19 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.18 ms /     4 runs   (    0.04 ms per token, 22222.22 tokens per second)\n",
            "llama_print_timings: prompt eval time =     541.63 ms /    18 tokens (   30.09 ms per token,    33.23 tokens per second)\n",
            "llama_print_timings:        eval time =     146.18 ms /     3 runs   (   48.73 ms per token,    20.52 tokens per second)\n",
            "llama_print_timings:       total time =     697.43 ms /    21 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.18 ms /     4 runs   (    0.05 ms per token, 21739.13 tokens per second)\n",
            "llama_print_timings: prompt eval time =     562.16 ms /    17 tokens (   33.07 ms per token,    30.24 tokens per second)\n",
            "llama_print_timings:        eval time =     169.30 ms /     3 runs   (   56.43 ms per token,    17.72 tokens per second)\n",
            "llama_print_timings:       total time =     741.77 ms /    20 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.24 ms /     6 runs   (    0.04 ms per token, 24590.16 tokens per second)\n",
            "llama_print_timings: prompt eval time =     455.19 ms /    16 tokens (   28.45 ms per token,    35.15 tokens per second)\n",
            "llama_print_timings:        eval time =     321.04 ms /     5 runs   (   64.21 ms per token,    15.57 tokens per second)\n",
            "llama_print_timings:       total time =     793.05 ms /    21 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.20 ms /     5 runs   (    0.04 ms per token, 24630.54 tokens per second)\n",
            "llama_print_timings: prompt eval time =     428.92 ms /    13 tokens (   32.99 ms per token,    30.31 tokens per second)\n",
            "llama_print_timings:        eval time =     267.01 ms /     4 runs   (   66.75 ms per token,    14.98 tokens per second)\n",
            "llama_print_timings:       total time =     708.73 ms /    17 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.18 ms /     4 runs   (    0.04 ms per token, 22727.27 tokens per second)\n",
            "llama_print_timings: prompt eval time =     551.40 ms /    17 tokens (   32.44 ms per token,    30.83 tokens per second)\n",
            "llama_print_timings:        eval time =     186.83 ms /     3 runs   (   62.28 ms per token,    16.06 tokens per second)\n",
            "llama_print_timings:       total time =     748.28 ms /    20 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.17 ms /     4 runs   (    0.04 ms per token, 22988.51 tokens per second)\n",
            "llama_print_timings: prompt eval time =     444.91 ms /    13 tokens (   34.22 ms per token,    29.22 tokens per second)\n",
            "llama_print_timings:        eval time =     185.28 ms /     3 runs   (   61.76 ms per token,    16.19 tokens per second)\n",
            "llama_print_timings:       total time =     639.79 ms /    16 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.17 ms /     4 runs   (    0.04 ms per token, 23121.39 tokens per second)\n",
            "llama_print_timings: prompt eval time =     582.58 ms /    17 tokens (   34.27 ms per token,    29.18 tokens per second)\n",
            "llama_print_timings:        eval time =     179.12 ms /     3 runs   (   59.71 ms per token,    16.75 tokens per second)\n",
            "llama_print_timings:       total time =     772.21 ms /    20 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.19 ms /     4 runs   (    0.05 ms per token, 21390.37 tokens per second)\n",
            "llama_print_timings: prompt eval time =     440.45 ms /    16 tokens (   27.53 ms per token,    36.33 tokens per second)\n",
            "llama_print_timings:        eval time =     186.42 ms /     3 runs   (   62.14 ms per token,    16.09 tokens per second)\n",
            "llama_print_timings:       total time =     636.68 ms /    19 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.20 ms /     5 runs   (    0.04 ms per token, 24752.48 tokens per second)\n",
            "llama_print_timings: prompt eval time =     580.43 ms /    17 tokens (   34.14 ms per token,    29.29 tokens per second)\n",
            "llama_print_timings:        eval time =     260.79 ms /     4 runs   (   65.20 ms per token,    15.34 tokens per second)\n",
            "llama_print_timings:       total time =     853.84 ms /    21 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.21 ms /     5 runs   (    0.04 ms per token, 23474.18 tokens per second)\n",
            "llama_print_timings: prompt eval time =     442.07 ms /    15 tokens (   29.47 ms per token,    33.93 tokens per second)\n",
            "llama_print_timings:        eval time =     279.22 ms /     4 runs   (   69.80 ms per token,    14.33 tokens per second)\n",
            "llama_print_timings:       total time =     735.16 ms /    19 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.17 ms /     4 runs   (    0.04 ms per token, 23809.52 tokens per second)\n",
            "llama_print_timings: prompt eval time =     557.38 ms /    18 tokens (   30.97 ms per token,    32.29 tokens per second)\n",
            "llama_print_timings:        eval time =     188.35 ms /     3 runs   (   62.78 ms per token,    15.93 tokens per second)\n",
            "llama_print_timings:       total time =     755.75 ms /    21 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.17 ms /     4 runs   (    0.04 ms per token, 24242.42 tokens per second)\n",
            "llama_print_timings: prompt eval time =     447.40 ms /    15 tokens (   29.83 ms per token,    33.53 tokens per second)\n",
            "llama_print_timings:        eval time =     174.68 ms /     3 runs   (   58.23 ms per token,    17.17 tokens per second)\n",
            "llama_print_timings:       total time =     632.20 ms /    18 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.17 ms /     4 runs   (    0.04 ms per token, 23668.64 tokens per second)\n",
            "llama_print_timings: prompt eval time =     604.70 ms /    17 tokens (   35.57 ms per token,    28.11 tokens per second)\n",
            "llama_print_timings:        eval time =     254.69 ms /     3 runs   (   84.90 ms per token,    11.78 tokens per second)\n",
            "llama_print_timings:       total time =     870.69 ms /    20 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.16 ms /     4 runs   (    0.04 ms per token, 24539.88 tokens per second)\n",
            "llama_print_timings: prompt eval time =     475.78 ms /    16 tokens (   29.74 ms per token,    33.63 tokens per second)\n",
            "llama_print_timings:        eval time =     297.73 ms /     3 runs   (   99.24 ms per token,    10.08 tokens per second)\n",
            "llama_print_timings:       total time =     783.47 ms /    19 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.25 ms /     6 runs   (    0.04 ms per token, 24390.24 tokens per second)\n",
            "llama_print_timings: prompt eval time =     461.56 ms /    15 tokens (   30.77 ms per token,    32.50 tokens per second)\n",
            "llama_print_timings:        eval time =     357.90 ms /     5 runs   (   71.58 ms per token,    13.97 tokens per second)\n",
            "llama_print_timings:       total time =     836.84 ms /    20 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.16 ms /     4 runs   (    0.04 ms per token, 24539.88 tokens per second)\n",
            "llama_print_timings: prompt eval time =     335.12 ms /    12 tokens (   27.93 ms per token,    35.81 tokens per second)\n",
            "llama_print_timings:        eval time =     189.50 ms /     3 runs   (   63.17 ms per token,    15.83 tokens per second)\n",
            "llama_print_timings:       total time =     534.33 ms /    15 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.17 ms /     4 runs   (    0.04 ms per token, 24242.42 tokens per second)\n",
            "llama_print_timings: prompt eval time =     466.62 ms /    15 tokens (   31.11 ms per token,    32.15 tokens per second)\n",
            "llama_print_timings:        eval time =     177.02 ms /     3 runs   (   59.01 ms per token,    16.95 tokens per second)\n",
            "llama_print_timings:       total time =     653.69 ms /    18 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.16 ms /     4 runs   (    0.04 ms per token, 24539.88 tokens per second)\n",
            "llama_print_timings: prompt eval time =     471.60 ms /    16 tokens (   29.48 ms per token,    33.93 tokens per second)\n",
            "llama_print_timings:        eval time =     181.16 ms /     3 runs   (   60.39 ms per token,    16.56 tokens per second)\n",
            "llama_print_timings:       total time =     663.54 ms /    19 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.26 ms /     6 runs   (    0.04 ms per token, 23166.02 tokens per second)\n",
            "llama_print_timings: prompt eval time =     454.61 ms /    16 tokens (   28.41 ms per token,    35.20 tokens per second)\n",
            "llama_print_timings:        eval time =     349.88 ms /     5 runs   (   69.98 ms per token,    14.29 tokens per second)\n",
            "llama_print_timings:       total time =     821.63 ms /    21 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.16 ms /     4 runs   (    0.04 ms per token, 24539.88 tokens per second)\n",
            "llama_print_timings: prompt eval time =     440.05 ms /    14 tokens (   31.43 ms per token,    31.81 tokens per second)\n",
            "llama_print_timings:        eval time =     182.28 ms /     3 runs   (   60.76 ms per token,    16.46 tokens per second)\n",
            "llama_print_timings:       total time =     633.77 ms /    17 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.19 ms /     4 runs   (    0.05 ms per token, 21052.63 tokens per second)\n",
            "llama_print_timings: prompt eval time =     573.01 ms /    17 tokens (   33.71 ms per token,    29.67 tokens per second)\n",
            "llama_print_timings:        eval time =     191.19 ms /     3 runs   (   63.73 ms per token,    15.69 tokens per second)\n",
            "llama_print_timings:       total time =     775.24 ms /    20 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.24 ms /     6 runs   (    0.04 ms per token, 24489.80 tokens per second)\n",
            "llama_print_timings: prompt eval time =     441.52 ms /    16 tokens (   27.60 ms per token,    36.24 tokens per second)\n",
            "llama_print_timings:        eval time =     361.50 ms /     5 runs   (   72.30 ms per token,    13.83 tokens per second)\n",
            "llama_print_timings:       total time =     819.67 ms /    21 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.24 ms /     5 runs   (    0.05 ms per token, 20920.50 tokens per second)\n",
            "llama_print_timings: prompt eval time =     552.22 ms /    17 tokens (   32.48 ms per token,    30.79 tokens per second)\n",
            "llama_print_timings:        eval time =     256.23 ms /     4 runs   (   64.06 ms per token,    15.61 tokens per second)\n",
            "llama_print_timings:       total time =     822.78 ms /    21 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.17 ms /     4 runs   (    0.04 ms per token, 23952.10 tokens per second)\n",
            "llama_print_timings: prompt eval time =     539.17 ms /    17 tokens (   31.72 ms per token,    31.53 tokens per second)\n",
            "llama_print_timings:        eval time =     196.57 ms /     3 runs   (   65.52 ms per token,    15.26 tokens per second)\n",
            "llama_print_timings:       total time =     746.72 ms /    20 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.18 ms /     4 runs   (    0.04 ms per token, 22727.27 tokens per second)\n",
            "llama_print_timings: prompt eval time =     551.22 ms /    17 tokens (   32.42 ms per token,    30.84 tokens per second)\n",
            "llama_print_timings:        eval time =     185.30 ms /     3 runs   (   61.77 ms per token,    16.19 tokens per second)\n",
            "llama_print_timings:       total time =     748.49 ms /    20 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.23 ms /     5 runs   (    0.05 ms per token, 21929.82 tokens per second)\n",
            "llama_print_timings: prompt eval time =     333.90 ms /    12 tokens (   27.82 ms per token,    35.94 tokens per second)\n",
            "llama_print_timings:        eval time =     241.49 ms /     4 runs   (   60.37 ms per token,    16.56 tokens per second)\n",
            "llama_print_timings:       total time =     590.08 ms /    16 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.17 ms /     4 runs   (    0.04 ms per token, 24096.39 tokens per second)\n",
            "llama_print_timings: prompt eval time =     462.50 ms /    16 tokens (   28.91 ms per token,    34.59 tokens per second)\n",
            "llama_print_timings:        eval time =     176.35 ms /     3 runs   (   58.78 ms per token,    17.01 tokens per second)\n",
            "llama_print_timings:       total time =     651.01 ms /    19 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.17 ms /     4 runs   (    0.04 ms per token, 23255.81 tokens per second)\n",
            "llama_print_timings: prompt eval time =     434.08 ms /    14 tokens (   31.01 ms per token,    32.25 tokens per second)\n",
            "llama_print_timings:        eval time =     180.02 ms /     3 runs   (   60.01 ms per token,    16.66 tokens per second)\n",
            "llama_print_timings:       total time =     625.75 ms /    17 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.21 ms /     5 runs   (    0.04 ms per token, 23809.52 tokens per second)\n",
            "llama_print_timings: prompt eval time =     556.88 ms /    18 tokens (   30.94 ms per token,    32.32 tokens per second)\n",
            "llama_print_timings:        eval time =     331.51 ms /     4 runs   (   82.88 ms per token,    12.07 tokens per second)\n",
            "llama_print_timings:       total time =     901.58 ms /    22 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.25 ms /     6 runs   (    0.04 ms per token, 24291.50 tokens per second)\n",
            "llama_print_timings: prompt eval time =     458.73 ms /    16 tokens (   28.67 ms per token,    34.88 tokens per second)\n",
            "llama_print_timings:        eval time =     481.89 ms /     5 runs   (   96.38 ms per token,    10.38 tokens per second)\n",
            "llama_print_timings:       total time =     956.53 ms /    21 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.21 ms /     5 runs   (    0.04 ms per token, 23584.91 tokens per second)\n",
            "llama_print_timings: prompt eval time =     556.76 ms /    18 tokens (   30.93 ms per token,    32.33 tokens per second)\n",
            "llama_print_timings:        eval time =     296.13 ms /     4 runs   (   74.03 ms per token,    13.51 tokens per second)\n",
            "llama_print_timings:       total time =     866.52 ms /    22 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.16 ms /     4 runs   (    0.04 ms per token, 24691.36 tokens per second)\n",
            "llama_print_timings: prompt eval time =     419.08 ms /    14 tokens (   29.93 ms per token,    33.41 tokens per second)\n",
            "llama_print_timings:        eval time =     170.36 ms /     3 runs   (   56.79 ms per token,    17.61 tokens per second)\n",
            "llama_print_timings:       total time =     599.21 ms /    17 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.25 ms /     6 runs   (    0.04 ms per token, 23904.38 tokens per second)\n",
            "llama_print_timings: prompt eval time =     462.22 ms /    16 tokens (   28.89 ms per token,    34.62 tokens per second)\n",
            "llama_print_timings:        eval time =     322.55 ms /     5 runs   (   64.51 ms per token,    15.50 tokens per second)\n",
            "llama_print_timings:       total time =     801.37 ms /    21 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.17 ms /     4 runs   (    0.04 ms per token, 24242.42 tokens per second)\n",
            "llama_print_timings: prompt eval time =     524.28 ms /    17 tokens (   30.84 ms per token,    32.43 tokens per second)\n",
            "llama_print_timings:        eval time =     168.83 ms /     3 runs   (   56.28 ms per token,    17.77 tokens per second)\n",
            "llama_print_timings:       total time =     702.92 ms /    20 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.21 ms /     5 runs   (    0.04 ms per token, 23923.44 tokens per second)\n",
            "llama_print_timings: prompt eval time =     442.65 ms /    15 tokens (   29.51 ms per token,    33.89 tokens per second)\n",
            "llama_print_timings:        eval time =     234.29 ms /     4 runs   (   58.57 ms per token,    17.07 tokens per second)\n",
            "llama_print_timings:       total time =     690.15 ms /    19 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.24 ms /     6 runs   (    0.04 ms per token, 24691.36 tokens per second)\n",
            "llama_print_timings: prompt eval time =     440.46 ms /    16 tokens (   27.53 ms per token,    36.33 tokens per second)\n",
            "llama_print_timings:        eval time =     319.84 ms /     5 runs   (   63.97 ms per token,    15.63 tokens per second)\n",
            "llama_print_timings:       total time =     776.37 ms /    21 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.17 ms /     4 runs   (    0.04 ms per token, 23809.52 tokens per second)\n",
            "llama_print_timings: prompt eval time =     421.72 ms /    13 tokens (   32.44 ms per token,    30.83 tokens per second)\n",
            "llama_print_timings:        eval time =     182.97 ms /     3 runs   (   60.99 ms per token,    16.40 tokens per second)\n",
            "llama_print_timings:       total time =     615.46 ms /    16 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.23 ms /     5 runs   (    0.05 ms per token, 22123.89 tokens per second)\n",
            "llama_print_timings: prompt eval time =     432.51 ms /    16 tokens (   27.03 ms per token,    36.99 tokens per second)\n",
            "llama_print_timings:        eval time =     231.69 ms /     4 runs   (   57.92 ms per token,    17.26 tokens per second)\n",
            "llama_print_timings:       total time =     677.12 ms /    20 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.20 ms /     5 runs   (    0.04 ms per token, 24630.54 tokens per second)\n",
            "llama_print_timings: prompt eval time =     439.93 ms /    15 tokens (   29.33 ms per token,    34.10 tokens per second)\n",
            "llama_print_timings:        eval time =     248.56 ms /     4 runs   (   62.14 ms per token,    16.09 tokens per second)\n",
            "llama_print_timings:       total time =     701.68 ms /    19 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.20 ms /     5 runs   (    0.04 ms per token, 25000.00 tokens per second)\n",
            "llama_print_timings: prompt eval time =     433.49 ms /    15 tokens (   28.90 ms per token,    34.60 tokens per second)\n",
            "llama_print_timings:        eval time =     255.30 ms /     4 runs   (   63.82 ms per token,    15.67 tokens per second)\n",
            "llama_print_timings:       total time =     701.23 ms /    19 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.21 ms /     5 runs   (    0.04 ms per token, 23474.18 tokens per second)\n",
            "llama_print_timings: prompt eval time =     436.68 ms /    16 tokens (   27.29 ms per token,    36.64 tokens per second)\n",
            "llama_print_timings:        eval time =     256.08 ms /     4 runs   (   64.02 ms per token,    15.62 tokens per second)\n",
            "llama_print_timings:       total time =     706.06 ms /    20 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.21 ms /     5 runs   (    0.04 ms per token, 23809.52 tokens per second)\n",
            "llama_print_timings: prompt eval time =     430.55 ms /    15 tokens (   28.70 ms per token,    34.84 tokens per second)\n",
            "llama_print_timings:        eval time =     245.55 ms /     4 runs   (   61.39 ms per token,    16.29 tokens per second)\n",
            "llama_print_timings:       total time =     690.01 ms /    19 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.17 ms /     4 runs   (    0.04 ms per token, 24096.39 tokens per second)\n",
            "llama_print_timings: prompt eval time =     526.03 ms /    17 tokens (   30.94 ms per token,    32.32 tokens per second)\n",
            "llama_print_timings:        eval time =     175.30 ms /     3 runs   (   58.43 ms per token,    17.11 tokens per second)\n",
            "llama_print_timings:       total time =     712.30 ms /    20 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.17 ms /     4 runs   (    0.04 ms per token, 23952.10 tokens per second)\n",
            "llama_print_timings: prompt eval time =     433.60 ms /    16 tokens (   27.10 ms per token,    36.90 tokens per second)\n",
            "llama_print_timings:        eval time =     170.33 ms /     3 runs   (   56.78 ms per token,    17.61 tokens per second)\n",
            "llama_print_timings:       total time =     614.26 ms /    19 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.25 ms /     6 runs   (    0.04 ms per token, 23715.42 tokens per second)\n",
            "llama_print_timings: prompt eval time =     432.89 ms /    16 tokens (   27.06 ms per token,    36.96 tokens per second)\n",
            "llama_print_timings:        eval time =     303.66 ms /     5 runs   (   60.73 ms per token,    16.47 tokens per second)\n",
            "llama_print_timings:       total time =     753.23 ms /    21 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.21 ms /     5 runs   (    0.04 ms per token, 24154.59 tokens per second)\n",
            "llama_print_timings: prompt eval time =     452.25 ms /    15 tokens (   30.15 ms per token,    33.17 tokens per second)\n",
            "llama_print_timings:        eval time =     452.40 ms /     4 runs   (  113.10 ms per token,     8.84 tokens per second)\n",
            "llama_print_timings:       total time =     918.49 ms /    19 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.17 ms /     4 runs   (    0.04 ms per token, 23255.81 tokens per second)\n",
            "llama_print_timings: prompt eval time =     332.18 ms /    12 tokens (   27.68 ms per token,    36.12 tokens per second)\n",
            "llama_print_timings:        eval time =     350.85 ms /     3 runs   (  116.95 ms per token,     8.55 tokens per second)\n",
            "llama_print_timings:       total time =     695.61 ms /    15 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.21 ms /     5 runs   (    0.04 ms per token, 23696.68 tokens per second)\n",
            "llama_print_timings: prompt eval time =     544.87 ms /    18 tokens (   30.27 ms per token,    33.04 tokens per second)\n",
            "llama_print_timings:        eval time =     313.91 ms /     4 runs   (   78.48 ms per token,    12.74 tokens per second)\n",
            "llama_print_timings:       total time =     872.02 ms /    22 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.17 ms /     4 runs   (    0.04 ms per token, 23809.52 tokens per second)\n",
            "llama_print_timings: prompt eval time =     416.38 ms /    14 tokens (   29.74 ms per token,    33.62 tokens per second)\n",
            "llama_print_timings:        eval time =     163.61 ms /     3 runs   (   54.54 ms per token,    18.34 tokens per second)\n",
            "llama_print_timings:       total time =     590.91 ms /    17 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.23 ms /     5 runs   (    0.05 ms per token, 21739.13 tokens per second)\n",
            "llama_print_timings: prompt eval time =     536.92 ms /    18 tokens (   29.83 ms per token,    33.52 tokens per second)\n",
            "llama_print_timings:        eval time =     252.51 ms /     4 runs   (   63.13 ms per token,    15.84 tokens per second)\n",
            "llama_print_timings:       total time =     801.70 ms /    22 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       0.16 ms /     4 runs   (    0.04 ms per token, 24390.24 tokens per second)\n",
            "llama_print_timings: prompt eval time =     537.81 ms /    20 tokens (   26.89 ms per token,    37.19 tokens per second)\n",
            "llama_print_timings:        eval time =     177.62 ms /     3 runs   (   59.21 ms per token,    16.89 tokens per second)\n",
            "llama_print_timings:       total time =     726.32 ms /    23 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking what the input review looked like before being sent to LLaMA\n",
        "i = 10\n",
        "print(data_1.loc[i, 'ReviewText'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEugBqp3zywG",
        "outputId": "4a5c7dff-30e1-4ad7-cc63-7e80c4da0237"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review #11: Plain narrative, suitable for casual reading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking what the input review looked like before being sent to LLaMA\n",
        "i = 10\n",
        "print(data_1.loc[i, 'model_response'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1KQGEMYeyKf",
        "outputId": "f6ff766e-f946-4c0f-f889-6db8b8810a8b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neutral\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract standardized sentiment label from LLaMA's raw text output\n",
        "\n",
        "def extract_sentiment(text):\n",
        "    text = str(text).lower()\n",
        "    if \"positive\" in text:\n",
        "        return \"Positive\"\n",
        "    elif \"negative\" in text:\n",
        "        return \"Negative\"\n",
        "    elif \"neutral\" in text:\n",
        "        return \"Neutral\"\n",
        "    else:\n",
        "        return \"Unknown\""
      ],
      "metadata": {
        "id": "u5OgwTTbCqSZ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying the function to the model response\n",
        "data_1['Sentiment_model_res'] = data_1['model_response'].apply(extract_sentiment)\n",
        "data_1['Sentiment_model_res'].head(11)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        },
        "id": "ig3PHN943OJn",
        "outputId": "1c94fe30-4918-4851-e8ea-14b83168a55e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     Negative\n",
              "1     Positive\n",
              "2     Positive\n",
              "3     Positive\n",
              "4      Neutral\n",
              "5     Positive\n",
              "6     Negative\n",
              "7      Neutral\n",
              "8      Neutral\n",
              "9      Neutral\n",
              "10     Neutral\n",
              "Name: Sentiment_model_res, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentiment_model_res</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This gives a quick overview of how many reviews were labeled as Positive, Neutral, Negative, or Unknown\n",
        "data_1['Sentiment_model_res'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "qo8YrnSD3ny5",
        "outputId": "ebe3c1ff-60e9-47f3-94d0-dd07222ab454"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sentiment_model_res\n",
              "Positive    37\n",
              "Neutral     37\n",
              "Negative    26\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Sentiment_model_res</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Positive</th>\n",
              "      <td>37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Neutral</th>\n",
              "      <td>37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Negative</th>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Debugging why the sentiment couldn't be extracted correctly\n",
        "data_1[data_1['Sentiment_model_res'] == 'Unknown']['model_response'].head(5).tolist()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4R8L6Tm_9R_",
        "outputId": "11908be2-f127-45c3-bd90-aa1bbac4fbd5"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Store the cleaned version of the dataset in final_data_1\n",
        "final_data_1 = data_1.drop(['model_response'], axis=1)\n",
        "final_data_1.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "0a6BLypJ8z4p",
        "outputId": "b2d1d07a-4342-4901-d19e-8d7fe5185005"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                          ReviewText  Sentiment  \\\n",
              "0              Review #1: Highly disappointing read.          0   \n",
              "1  Review #2: A page-turner with a powerful message.          2   \n",
              "2          Review #3: A masterpiece of storytelling.          2   \n",
              "3        Review #4: Heartwarming and inspiring read.          2   \n",
              "4        Review #5: Neither good nor bad, just fine.          1   \n",
              "\n",
              "  Sentiment_model_res  \n",
              "0            Negative  \n",
              "1            Positive  \n",
              "2            Positive  \n",
              "3            Positive  \n",
              "4             Neutral  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c1902f07-5810-4783-a7b8-3fd18364591f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ReviewText</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>Sentiment_model_res</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Review #1: Highly disappointing read.</td>\n",
              "      <td>0</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Review #2: A page-turner with a powerful message.</td>\n",
              "      <td>2</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Review #3: A masterpiece of storytelling.</td>\n",
              "      <td>2</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Review #4: Heartwarming and inspiring read.</td>\n",
              "      <td>2</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Review #5: Neither good nor bad, just fine.</td>\n",
              "      <td>1</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c1902f07-5810-4783-a7b8-3fd18364591f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c1902f07-5810-4783-a7b8-3fd18364591f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c1902f07-5810-4783-a7b8-3fd18364591f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-e80d1c9f-3207-4ccb-b211-b3a5b93f9384\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e80d1c9f-3207-4ccb-b211-b3a5b93f9384')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-e80d1c9f-3207-4ccb-b211-b3a5b93f9384 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "final_data_1",
              "summary": "{\n  \"name\": \"final_data_1\",\n  \"rows\": 100,\n  \"fields\": [\n    {\n      \"column\": \"ReviewText\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"Review #84: Uninspiring and boring.\",\n          \"Review #54: The best book I've read this year.\",\n          \"Review #71: Neither good nor bad, just fine.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sentiment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0,\n          2,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sentiment_model_res\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Negative\",\n          \"Positive\",\n          \"Neutral\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LLaMA vs. True Sentiment Labels: Evaluation of Prediction Accuracy"
      ],
      "metadata": {
        "id": "7ys6M2RDz8MB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Normalize predictions\n",
        "data_1['model_response'] = data_1['model_response'].str.strip().str.capitalize()\n",
        "\n",
        "# Map LLaMA predictions to numeric values\n",
        "sentiment_map = {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n",
        "data_1['Sentiment_model_res'] = data_1['model_response'].map(sentiment_map)\n",
        "\n",
        "# Drop invalid predictions\n",
        "data_eval = data_1.dropna(subset=['Sentiment_model_res'])\n",
        "\n",
        "y_true = data_eval['Sentiment_model_res']\n",
        "y_pred = data_eval['Sentiment_model_res']\n",
        "\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "report = classification_report(y_true, y_pred, target_names=['Negative', 'Neutral', 'Positive'])\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"\\nClassification Report:\\n\", report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJAfiYkFxeQD",
        "outputId": "5d7a4f80-2bd3-411a-b086-c1e609dd37c9"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    Negative       1.00      1.00      1.00        22\n",
            "     Neutral       1.00      1.00      1.00        24\n",
            "    Positive       1.00      1.00      1.00        31\n",
            "\n",
            "    accuracy                           1.00        77\n",
            "   macro avg       1.00      1.00      1.00        77\n",
            "weighted avg       1.00      1.00      1.00        77\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary:**\n",
        "- Accuracy: 1.0: Indicates the model predicted every label correctly"
      ],
      "metadata": {
        "id": "EMPA09t94bvg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Map numeric sentiment to label\n",
        "sentiment_labels = {0: 'Negative', 1: 'Neutral', 2: 'Positive'}\n",
        "labelled_counts = data_1['Sentiment_model_res'].map(sentiment_labels).value_counts()\n",
        "total = labelled_counts.sum()\n",
        "\n",
        "# Plot\n",
        "labelled_counts.plot(kind='bar', color='skyblue')\n",
        "plt.title(\"Model–Predicted Sentiment Distribution\")\n",
        "plt.ylabel(\"Number of Reviews\")\n",
        "plt.xticks(rotation=0)\n",
        "plt.grid(axis='y')\n",
        "\n",
        "# Add count and percentage on top of bars\n",
        "for i, (label, count) in enumerate(labelled_counts.items()):\n",
        "    percent = (count / total) * 100\n",
        "    plt.text(i, count - 4, f'{count} ({percent:.1f}%)', ha='center', fontsize=10)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "D4AWAU8S9nai",
        "outputId": "af74e75f-9821-427c-e4bb-d3f3787788cc"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Overall Sentiment Classification Using LLaMA:**\n",
        "\n",
        "In the first stage of the project, I used a Large Language Model (LLaMA) to classify the overall sentiment of book reviews as Positive, Neutral, or Negative. Each review was passed through the model using a structured prompt designed to restrict the output to a single sentiment label.\n",
        "\n",
        "The raw predictions were then cleaned and standardized using a custom parsing function to ensure consistency across all outputs."
      ],
      "metadata": {
        "id": "FQXmRAtuYUrH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Understanding Reader Sentiment with LLaMA: Overall Sentiment, Aspect-Level Insights, and Liked/Disliked Features"
      ],
      "metadata": {
        "id": "RX5JjiOe5ZFW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a copy of the first 200 rows from the original dataset for faster processing\n",
        "data_4 = data.iloc[:100].copy()"
      ],
      "metadata": {
        "id": "74VaoNfr5Yqi"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining the instructions for the model\n",
        "\n",
        "instruction_2 = \"\"\"\n",
        "You are an AI analyzing book reviews.\n",
        "\n",
        "Your task is to:\n",
        "1. Classify the **overall sentiment** of the review as one of the following **exact** words:\n",
        "   - \"Positive\"\n",
        "   - \"Negative\"\n",
        "   - \"Neutral\"\n",
        "\n",
        "2. Determine the sentiment of the following aspects of the book using one of:\n",
        "   - \"Positive\"\n",
        "   - \"Negative\"\n",
        "   - \"Neutral\"\n",
        "   - \"Not Applicable\" (if not mentioned)\n",
        "\n",
        "   Aspects:\n",
        "   - Writing Style\n",
        "   - Emotional Impact\n",
        "\n",
        "3. Identify liked and/or disliked features for the following:\n",
        "   - Pacing\n",
        "   - Ending\n",
        "\n",
        "Return a list of quoted features for each. If no features are found, return an empty list.\n",
        "\n",
        "4. Write a polite, empathetic response to the customer. Begin with a thank you. Then:\n",
        "   - If **Positive**, say you’re glad they enjoyed the book and would love to have them read again.\n",
        "   - If **Neutral**, thank them and ask what could have improved their experience.\n",
        "   - If **Negative**, apologize and say their feedback will be reviewed.\n",
        "\n",
        "Output only the **exact JSON object below** — nothing else. All values must be strings. Lists must contain strings in double quotes.\n",
        "\n",
        "{\n",
        "    \"Overall Sentiment\": \"your_sentiment_prediction\",\n",
        "    \"Writing Style\": \"your_sentiment_prediction\",\n",
        "    \"Emotional Impact\": \"your_sentiment_prediction\",\n",
        "    \"Pacing\": [\"liked or disliked features\"],\n",
        "    \"Ending\": [\"liked or disliked features\"],\n",
        "    \"Response\": \"your_response_to_the_customer_review\"\n",
        "}\n",
        "\"\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "TBhlayz5Kpjv"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the LLaMA model to each review and generate a sentiment prediction\n",
        "tqdm.pandas()\n",
        "data_4['model_response'] = data_4['ReviewText'].progress_apply(\n",
        "    lambda x: generate_llama_response(instruction_2, x)\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f2aeddb3fa3a4410b66e7f7769384b66",
            "20fb77f5549e49039ee58ad38e2d8e1a",
            "608c9fc55ae548b1852c41b125b6f7ee",
            "c41aa8bfdf4e4053bd6f7782dd177700",
            "b145860238484c9db407c41f3561159b",
            "d16d07a74e4c489391e363856dbf7372",
            "681e6701763e4a65964bfc90f241082a",
            "fe11730f79194bb1b6627ef9a6b0bf83",
            "85abf3242d9348368081456b575acb58",
            "c84e8a108a2e4a05929e0299d9425300",
            "efc829574cec419bacfc7dd8c4333451"
          ]
        },
        "id": "ifQjhn6v5569",
        "outputId": "491848b7-9701-4415-f00d-fcf8f638b5ba"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f2aeddb3fa3a4410b66e7f7769384b66"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =      13.09 ms /   315 runs   (    0.04 ms per token, 24064.17 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1012.92 ms /   444 tokens (    2.28 ms per token,   438.33 tokens per second)\n",
            "llama_print_timings:        eval time =   23699.80 ms /   314 runs   (   75.48 ms per token,    13.25 tokens per second)\n",
            "llama_print_timings:       total time =   25679.93 ms /   758 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =      19.09 ms /   444 runs   (    0.04 ms per token, 23253.38 tokens per second)\n",
            "llama_print_timings: prompt eval time =     560.70 ms /    17 tokens (   32.98 ms per token,    30.32 tokens per second)\n",
            "llama_print_timings:        eval time =   34207.97 ms /   443 runs   (   77.22 ms per token,    12.95 tokens per second)\n",
            "llama_print_timings:       total time =   36160.80 ms /   460 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =      16.31 ms /   386 runs   (    0.04 ms per token, 23670.82 tokens per second)\n",
            "llama_print_timings: prompt eval time =     422.48 ms /    16 tokens (   26.41 ms per token,    37.87 tokens per second)\n",
            "llama_print_timings:        eval time =   27022.90 ms /   385 runs   (   70.19 ms per token,    14.25 tokens per second)\n",
            "llama_print_timings:       total time =   28721.49 ms /   401 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =      14.66 ms /   346 runs   (    0.04 ms per token, 23606.47 tokens per second)\n",
            "llama_print_timings: prompt eval time =     457.16 ms /    16 tokens (   28.57 ms per token,    35.00 tokens per second)\n",
            "llama_print_timings:        eval time =   26271.55 ms /   345 runs   (   76.15 ms per token,    13.13 tokens per second)\n",
            "llama_print_timings:       total time =   27971.67 ms /   361 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =      14.13 ms /   335 runs   (    0.04 ms per token, 23715.14 tokens per second)\n",
            "llama_print_timings: prompt eval time =     449.93 ms /    16 tokens (   28.12 ms per token,    35.56 tokens per second)\n",
            "llama_print_timings:        eval time =   24784.54 ms /   334 runs   (   74.21 ms per token,    13.48 tokens per second)\n",
            "llama_print_timings:       total time =   26350.70 ms /   350 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       4.98 ms /   117 runs   (    0.04 ms per token, 23512.86 tokens per second)\n",
            "llama_print_timings: prompt eval time =     418.08 ms /    14 tokens (   29.86 ms per token,    33.49 tokens per second)\n",
            "llama_print_timings:        eval time =    8608.90 ms /   116 runs   (   74.21 ms per token,    13.47 tokens per second)\n",
            "llama_print_timings:       total time =    9372.47 ms /   130 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =      13.47 ms /   319 runs   (    0.04 ms per token, 23676.98 tokens per second)\n",
            "llama_print_timings: prompt eval time =     537.43 ms /    17 tokens (   31.61 ms per token,    31.63 tokens per second)\n",
            "llama_print_timings:        eval time =   22995.33 ms /   318 runs   (   72.31 ms per token,    13.83 tokens per second)\n",
            "llama_print_timings:       total time =   24556.97 ms /   335 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =      12.83 ms /   307 runs   (    0.04 ms per token, 23932.02 tokens per second)\n",
            "llama_print_timings: prompt eval time =     439.37 ms /    15 tokens (   29.29 ms per token,    34.14 tokens per second)\n",
            "llama_print_timings:        eval time =   21401.17 ms /   306 runs   (   69.94 ms per token,    14.30 tokens per second)\n",
            "llama_print_timings:       total time =   22799.57 ms /   321 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =      16.50 ms /   393 runs   (    0.04 ms per token, 23822.51 tokens per second)\n",
            "llama_print_timings: prompt eval time =     450.62 ms /    15 tokens (   30.04 ms per token,    33.29 tokens per second)\n",
            "llama_print_timings:        eval time =   29075.31 ms /   392 runs   (   74.17 ms per token,    13.48 tokens per second)\n",
            "llama_print_timings:       total time =   30858.19 ms /   407 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       5.20 ms /   122 runs   (    0.04 ms per token, 23461.54 tokens per second)\n",
            "llama_print_timings: prompt eval time =     527.95 ms /    18 tokens (   29.33 ms per token,    34.09 tokens per second)\n",
            "llama_print_timings:        eval time =    8869.96 ms /   121 runs   (   73.31 ms per token,    13.64 tokens per second)\n",
            "llama_print_timings:       total time =    9820.10 ms /   139 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       6.87 ms /   161 runs   (    0.04 ms per token, 23442.05 tokens per second)\n",
            "llama_print_timings: prompt eval time =     536.43 ms /    18 tokens (   29.80 ms per token,    33.56 tokens per second)\n",
            "llama_print_timings:        eval time =   11605.46 ms /   160 runs   (   72.53 ms per token,    13.79 tokens per second)\n",
            "llama_print_timings:       total time =   12667.02 ms /   178 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       8.25 ms /   191 runs   (    0.04 ms per token, 23140.30 tokens per second)\n",
            "llama_print_timings: prompt eval time =     422.58 ms /    14 tokens (   30.18 ms per token,    33.13 tokens per second)\n",
            "llama_print_timings:        eval time =   13602.96 ms /   190 runs   (   71.59 ms per token,    13.97 tokens per second)\n",
            "llama_print_timings:       total time =   14759.89 ms /   204 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =      15.78 ms /   371 runs   (    0.04 ms per token, 23516.73 tokens per second)\n",
            "llama_print_timings: prompt eval time =     413.41 ms /    15 tokens (   27.56 ms per token,    36.28 tokens per second)\n",
            "llama_print_timings:        eval time =   27572.33 ms /   370 runs   (   74.52 ms per token,    13.42 tokens per second)\n",
            "llama_print_timings:       total time =   29314.11 ms /   385 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       6.03 ms /   142 runs   (    0.04 ms per token, 23537.21 tokens per second)\n",
            "llama_print_timings: prompt eval time =     425.57 ms /    16 tokens (   26.60 ms per token,    37.60 tokens per second)\n",
            "llama_print_timings:        eval time =   10291.13 ms /   141 runs   (   72.99 ms per token,    13.70 tokens per second)\n",
            "llama_print_timings:       total time =   11172.24 ms /   157 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       5.69 ms /   135 runs   (    0.04 ms per token, 23738.35 tokens per second)\n",
            "llama_print_timings: prompt eval time =     542.69 ms /    17 tokens (   31.92 ms per token,    31.33 tokens per second)\n",
            "llama_print_timings:        eval time =    9814.75 ms /   134 runs   (   73.24 ms per token,    13.65 tokens per second)\n",
            "llama_print_timings:       total time =   10786.05 ms /   151 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       6.82 ms /   161 runs   (    0.04 ms per token, 23593.20 tokens per second)\n",
            "llama_print_timings: prompt eval time =     529.74 ms /    18 tokens (   29.43 ms per token,    33.98 tokens per second)\n",
            "llama_print_timings:        eval time =   11562.07 ms /   160 runs   (   72.26 ms per token,    13.84 tokens per second)\n",
            "llama_print_timings:       total time =   12697.70 ms /   178 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =      14.81 ms /   350 runs   (    0.04 ms per token, 23629.49 tokens per second)\n",
            "llama_print_timings: prompt eval time =     455.63 ms /    15 tokens (   30.38 ms per token,    32.92 tokens per second)\n",
            "llama_print_timings:        eval time =   25261.87 ms /   349 runs   (   72.38 ms per token,    13.82 tokens per second)\n",
            "llama_print_timings:       total time =   26992.07 ms /   364 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =      14.14 ms /   332 runs   (    0.04 ms per token, 23479.49 tokens per second)\n",
            "llama_print_timings: prompt eval time =     434.60 ms /    16 tokens (   27.16 ms per token,    36.82 tokens per second)\n",
            "llama_print_timings:        eval time =   23638.98 ms /   331 runs   (   71.42 ms per token,    14.00 tokens per second)\n",
            "llama_print_timings:       total time =   25281.93 ms /   347 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       5.54 ms /   130 runs   (    0.04 ms per token, 23478.42 tokens per second)\n",
            "llama_print_timings: prompt eval time =     431.50 ms /    15 tokens (   28.77 ms per token,    34.76 tokens per second)\n",
            "llama_print_timings:        eval time =    9521.38 ms /   129 runs   (   73.81 ms per token,    13.55 tokens per second)\n",
            "llama_print_timings:       total time =   10400.95 ms /   144 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       5.70 ms /   134 runs   (    0.04 ms per token, 23508.77 tokens per second)\n",
            "llama_print_timings: prompt eval time =     433.24 ms /    14 tokens (   30.95 ms per token,    32.31 tokens per second)\n",
            "llama_print_timings:        eval time =    9709.72 ms /   133 runs   (   73.01 ms per token,    13.70 tokens per second)\n",
            "llama_print_timings:       total time =   10579.42 ms /   147 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       4.91 ms /   117 runs   (    0.04 ms per token, 23848.35 tokens per second)\n",
            "llama_print_timings: prompt eval time =     427.41 ms /    14 tokens (   30.53 ms per token,    32.76 tokens per second)\n",
            "llama_print_timings:        eval time =    8707.72 ms /   116 runs   (   75.07 ms per token,    13.32 tokens per second)\n",
            "llama_print_timings:       total time =    9526.35 ms /   130 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       5.45 ms /   128 runs   (    0.04 ms per token, 23503.49 tokens per second)\n",
            "llama_print_timings: prompt eval time =     541.28 ms /    17 tokens (   31.84 ms per token,    31.41 tokens per second)\n",
            "llama_print_timings:        eval time =    9223.41 ms /   127 runs   (   72.63 ms per token,    13.77 tokens per second)\n",
            "llama_print_timings:       total time =   10142.90 ms /   144 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       5.60 ms /   134 runs   (    0.04 ms per token, 23920.03 tokens per second)\n",
            "llama_print_timings: prompt eval time =     448.64 ms /    16 tokens (   28.04 ms per token,    35.66 tokens per second)\n",
            "llama_print_timings:        eval time =    9159.72 ms /   133 runs   (   68.87 ms per token,    14.52 tokens per second)\n",
            "llama_print_timings:       total time =   10028.76 ms /   149 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       5.43 ms /   128 runs   (    0.04 ms per token, 23581.43 tokens per second)\n",
            "llama_print_timings: prompt eval time =     532.91 ms /    17 tokens (   31.35 ms per token,    31.90 tokens per second)\n",
            "llama_print_timings:        eval time =    9452.38 ms /   127 runs   (   74.43 ms per token,    13.44 tokens per second)\n",
            "llama_print_timings:       total time =   10394.09 ms /   144 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       5.98 ms /   138 runs   (    0.04 ms per token, 23080.78 tokens per second)\n",
            "llama_print_timings: prompt eval time =     428.58 ms /    15 tokens (   28.57 ms per token,    35.00 tokens per second)\n",
            "llama_print_timings:        eval time =    9949.68 ms /   137 runs   (   72.63 ms per token,    13.77 tokens per second)\n",
            "llama_print_timings:       total time =   10793.06 ms /   152 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =      17.70 ms /   415 runs   (    0.04 ms per token, 23445.00 tokens per second)\n",
            "llama_print_timings: prompt eval time =     531.03 ms /    17 tokens (   31.24 ms per token,    32.01 tokens per second)\n",
            "llama_print_timings:        eval time =   31028.67 ms /   414 runs   (   74.95 ms per token,    13.34 tokens per second)\n",
            "llama_print_timings:       total time =   33011.40 ms /   431 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       5.91 ms /   137 runs   (    0.04 ms per token, 23196.75 tokens per second)\n",
            "llama_print_timings: prompt eval time =     513.46 ms /    17 tokens (   30.20 ms per token,    33.11 tokens per second)\n",
            "llama_print_timings:        eval time =    9964.70 ms /   136 runs   (   73.27 ms per token,    13.65 tokens per second)\n",
            "llama_print_timings:       total time =   10899.26 ms /   153 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       5.83 ms /   136 runs   (    0.04 ms per token, 23327.62 tokens per second)\n",
            "llama_print_timings: prompt eval time =     418.10 ms /    15 tokens (   27.87 ms per token,    35.88 tokens per second)\n",
            "llama_print_timings:        eval time =    9265.52 ms /   135 runs   (   68.63 ms per token,    14.57 tokens per second)\n",
            "llama_print_timings:       total time =   10101.96 ms /   150 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       7.30 ms /   171 runs   (    0.04 ms per token, 23411.83 tokens per second)\n",
            "llama_print_timings: prompt eval time =     447.48 ms /    15 tokens (   29.83 ms per token,    33.52 tokens per second)\n",
            "llama_print_timings:        eval time =   12341.22 ms /   170 runs   (   72.60 ms per token,    13.77 tokens per second)\n",
            "llama_print_timings:       total time =   13331.41 ms /   185 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =      12.99 ms /   307 runs   (    0.04 ms per token, 23626.29 tokens per second)\n",
            "llama_print_timings: prompt eval time =     449.22 ms /    16 tokens (   28.08 ms per token,    35.62 tokens per second)\n",
            "llama_print_timings:        eval time =   21958.36 ms /   306 runs   (   71.76 ms per token,    13.94 tokens per second)\n",
            "llama_print_timings:       total time =   23540.40 ms /   322 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       5.53 ms /   132 runs   (    0.04 ms per token, 23891.40 tokens per second)\n",
            "llama_print_timings: prompt eval time =     556.33 ms /    18 tokens (   30.91 ms per token,    32.35 tokens per second)\n",
            "llama_print_timings:        eval time =    9388.91 ms /   131 runs   (   71.67 ms per token,    13.95 tokens per second)\n",
            "llama_print_timings:       total time =   10380.16 ms /   149 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =      14.80 ms /   342 runs   (    0.04 ms per token, 23103.42 tokens per second)\n",
            "llama_print_timings: prompt eval time =     446.14 ms /    16 tokens (   27.88 ms per token,    35.86 tokens per second)\n",
            "llama_print_timings:        eval time =   24742.93 ms /   341 runs   (   72.56 ms per token,    13.78 tokens per second)\n",
            "llama_print_timings:       total time =   26386.33 ms /   357 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =      16.74 ms /   393 runs   (    0.04 ms per token, 23472.50 tokens per second)\n",
            "llama_print_timings: prompt eval time =     468.60 ms /    15 tokens (   31.24 ms per token,    32.01 tokens per second)\n",
            "llama_print_timings:        eval time =   28743.33 ms /   392 runs   (   73.32 ms per token,    13.64 tokens per second)\n",
            "llama_print_timings:       total time =   30703.74 ms /   407 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =      14.62 ms /   342 runs   (    0.04 ms per token, 23386.21 tokens per second)\n",
            "llama_print_timings: prompt eval time =     435.81 ms /    16 tokens (   27.24 ms per token,    36.71 tokens per second)\n",
            "llama_print_timings:        eval time =   24435.87 ms /   341 runs   (   71.66 ms per token,    13.95 tokens per second)\n",
            "llama_print_timings:       total time =   26171.29 ms /   357 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =      15.30 ms /   364 runs   (    0.04 ms per token, 23784.63 tokens per second)\n",
            "llama_print_timings: prompt eval time =     437.36 ms /    14 tokens (   31.24 ms per token,    32.01 tokens per second)\n",
            "llama_print_timings:        eval time =   26804.22 ms /   363 runs   (   73.84 ms per token,    13.54 tokens per second)\n",
            "llama_print_timings:       total time =   28591.87 ms /   377 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       5.68 ms /   135 runs   (    0.04 ms per token, 23780.17 tokens per second)\n",
            "llama_print_timings: prompt eval time =     503.48 ms /    17 tokens (   29.62 ms per token,    33.77 tokens per second)\n",
            "llama_print_timings:        eval time =    9883.79 ms /   134 runs   (   73.76 ms per token,    13.56 tokens per second)\n",
            "llama_print_timings:       total time =   10868.30 ms /   151 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       5.71 ms /   136 runs   (    0.04 ms per token, 23826.21 tokens per second)\n",
            "llama_print_timings: prompt eval time =     428.68 ms /    15 tokens (   28.58 ms per token,    34.99 tokens per second)\n",
            "llama_print_timings:        eval time =    9815.98 ms /   135 runs   (   72.71 ms per token,    13.75 tokens per second)\n",
            "llama_print_timings:       total time =   10730.21 ms /   150 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =      14.57 ms /   347 runs   (    0.04 ms per token, 23809.52 tokens per second)\n",
            "llama_print_timings: prompt eval time =     446.42 ms /    14 tokens (   31.89 ms per token,    31.36 tokens per second)\n",
            "llama_print_timings:        eval time =   25078.69 ms /   346 runs   (   72.48 ms per token,    13.80 tokens per second)\n",
            "llama_print_timings:       total time =   26842.44 ms /   360 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       5.51 ms /   128 runs   (    0.04 ms per token, 23238.93 tokens per second)\n",
            "llama_print_timings: prompt eval time =     524.08 ms /    17 tokens (   30.83 ms per token,    32.44 tokens per second)\n",
            "llama_print_timings:        eval time =    9239.55 ms /   127 runs   (   72.75 ms per token,    13.75 tokens per second)\n",
            "llama_print_timings:       total time =   10205.64 ms /   144 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       5.38 ms /   129 runs   (    0.04 ms per token, 23986.61 tokens per second)\n",
            "llama_print_timings: prompt eval time =     525.59 ms /    17 tokens (   30.92 ms per token,    32.34 tokens per second)\n",
            "llama_print_timings:        eval time =    9348.00 ms /   128 runs   (   73.03 ms per token,    13.69 tokens per second)\n",
            "llama_print_timings:       total time =   10322.10 ms /   145 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       5.44 ms /   129 runs   (    0.04 ms per token, 23704.52 tokens per second)\n",
            "llama_print_timings: prompt eval time =     435.81 ms /    16 tokens (   27.24 ms per token,    36.71 tokens per second)\n",
            "llama_print_timings:        eval time =    9411.70 ms /   128 runs   (   73.53 ms per token,    13.60 tokens per second)\n",
            "llama_print_timings:       total time =   10388.50 ms /   144 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =      14.18 ms /   332 runs   (    0.04 ms per token, 23406.66 tokens per second)\n",
            "llama_print_timings: prompt eval time =     431.02 ms /    16 tokens (   26.94 ms per token,    37.12 tokens per second)\n",
            "llama_print_timings:        eval time =   23713.77 ms /   331 runs   (   71.64 ms per token,    13.96 tokens per second)\n",
            "llama_print_timings:       total time =   25383.88 ms /   347 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       5.18 ms /   122 runs   (    0.04 ms per token, 23565.77 tokens per second)\n",
            "llama_print_timings: prompt eval time =     432.70 ms /    16 tokens (   27.04 ms per token,    36.98 tokens per second)\n",
            "llama_print_timings:        eval time =    8544.46 ms /   121 runs   (   70.62 ms per token,    14.16 tokens per second)\n",
            "llama_print_timings:       total time =    9411.56 ms /   137 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       5.48 ms /   129 runs   (    0.04 ms per token, 23535.85 tokens per second)\n",
            "llama_print_timings: prompt eval time =     337.45 ms /    12 tokens (   28.12 ms per token,    35.56 tokens per second)\n",
            "llama_print_timings:        eval time =    8636.95 ms /   128 runs   (   67.48 ms per token,    14.82 tokens per second)\n",
            "llama_print_timings:       total time =    9435.65 ms /   140 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       5.44 ms /   129 runs   (    0.04 ms per token, 23730.68 tokens per second)\n",
            "llama_print_timings: prompt eval time =     424.01 ms /    16 tokens (   26.50 ms per token,    37.74 tokens per second)\n",
            "llama_print_timings:        eval time =    9216.40 ms /   128 runs   (   72.00 ms per token,    13.89 tokens per second)\n",
            "llama_print_timings:       total time =   10136.26 ms /   144 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =      17.42 ms /   411 runs   (    0.04 ms per token, 23589.51 tokens per second)\n",
            "llama_print_timings: prompt eval time =     522.79 ms /    18 tokens (   29.04 ms per token,    34.43 tokens per second)\n",
            "llama_print_timings:        eval time =   30302.16 ms /   410 runs   (   73.91 ms per token,    13.53 tokens per second)\n",
            "llama_print_timings:       total time =   32375.61 ms /   428 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =      14.06 ms /   332 runs   (    0.04 ms per token, 23606.37 tokens per second)\n",
            "llama_print_timings: prompt eval time =     444.47 ms /    16 tokens (   27.78 ms per token,    36.00 tokens per second)\n",
            "llama_print_timings:        eval time =   24594.35 ms /   331 runs   (   74.30 ms per token,    13.46 tokens per second)\n",
            "llama_print_timings:       total time =   26267.90 ms /   347 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       5.79 ms /   137 runs   (    0.04 ms per token, 23649.23 tokens per second)\n",
            "llama_print_timings: prompt eval time =     528.22 ms /    17 tokens (   31.07 ms per token,    32.18 tokens per second)\n",
            "llama_print_timings:        eval time =    9648.87 ms /   136 runs   (   70.95 ms per token,    14.09 tokens per second)\n",
            "llama_print_timings:       total time =   10636.40 ms /   153 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =      16.26 ms /   384 runs   (    0.04 ms per token, 23617.69 tokens per second)\n",
            "llama_print_timings: prompt eval time =     446.36 ms /    15 tokens (   29.76 ms per token,    33.61 tokens per second)\n",
            "llama_print_timings:        eval time =   27545.03 ms /   383 runs   (   71.92 ms per token,    13.90 tokens per second)\n",
            "llama_print_timings:       total time =   29403.86 ms /   398 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       5.72 ms /   135 runs   (    0.04 ms per token, 23589.03 tokens per second)\n",
            "llama_print_timings: prompt eval time =     536.47 ms /    18 tokens (   29.80 ms per token,    33.55 tokens per second)\n",
            "llama_print_timings:        eval time =    9728.13 ms /   134 runs   (   72.60 ms per token,    13.77 tokens per second)\n",
            "llama_print_timings:       total time =   10742.11 ms /   152 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       5.86 ms /   137 runs   (    0.04 ms per token, 23382.83 tokens per second)\n",
            "llama_print_timings: prompt eval time =     538.48 ms /    17 tokens (   31.68 ms per token,    31.57 tokens per second)\n",
            "llama_print_timings:        eval time =    9890.71 ms /   136 runs   (   72.73 ms per token,    13.75 tokens per second)\n",
            "llama_print_timings:       total time =   10928.59 ms /   153 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =      14.06 ms /   332 runs   (    0.04 ms per token, 23611.41 tokens per second)\n",
            "llama_print_timings: prompt eval time =     438.67 ms /    16 tokens (   27.42 ms per token,    36.47 tokens per second)\n",
            "llama_print_timings:        eval time =   23737.22 ms /   331 runs   (   71.71 ms per token,    13.94 tokens per second)\n",
            "llama_print_timings:       total time =   25480.76 ms /   347 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       5.32 ms /   126 runs   (    0.04 ms per token, 23675.31 tokens per second)\n",
            "llama_print_timings: prompt eval time =     420.62 ms /    13 tokens (   32.36 ms per token,    30.91 tokens per second)\n",
            "llama_print_timings:        eval time =    9105.87 ms /   125 runs   (   72.85 ms per token,    13.73 tokens per second)\n",
            "llama_print_timings:       total time =   10008.90 ms /   138 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =      13.70 ms /   323 runs   (    0.04 ms per token, 23585.25 tokens per second)\n",
            "llama_print_timings: prompt eval time =     512.42 ms /    17 tokens (   30.14 ms per token,    33.18 tokens per second)\n",
            "llama_print_timings:        eval time =   23330.95 ms /   322 runs   (   72.46 ms per token,    13.80 tokens per second)\n",
            "llama_print_timings:       total time =   25067.84 ms /   339 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       5.44 ms /   129 runs   (    0.04 ms per token, 23708.88 tokens per second)\n",
            "llama_print_timings: prompt eval time =     431.23 ms /    13 tokens (   33.17 ms per token,    30.15 tokens per second)\n",
            "llama_print_timings:        eval time =    8807.09 ms /   128 runs   (   68.81 ms per token,    14.53 tokens per second)\n",
            "llama_print_timings:       total time =    9696.88 ms /   141 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       5.76 ms /   135 runs   (    0.04 ms per token, 23441.57 tokens per second)\n",
            "llama_print_timings: prompt eval time =     539.61 ms /    17 tokens (   31.74 ms per token,    31.50 tokens per second)\n",
            "llama_print_timings:        eval time =    9460.22 ms /   134 runs   (   70.60 ms per token,    14.16 tokens per second)\n",
            "llama_print_timings:       total time =   10531.86 ms /   151 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       5.14 ms /   120 runs   (    0.04 ms per token, 23328.15 tokens per second)\n",
            "llama_print_timings: prompt eval time =     433.19 ms /    16 tokens (   27.07 ms per token,    36.94 tokens per second)\n",
            "llama_print_timings:        eval time =    8755.05 ms /   119 runs   (   73.57 ms per token,    13.59 tokens per second)\n",
            "llama_print_timings:       total time =    9592.73 ms /   135 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =      14.38 ms /   342 runs   (    0.04 ms per token, 23778.07 tokens per second)\n",
            "llama_print_timings: prompt eval time =     525.81 ms /    17 tokens (   30.93 ms per token,    32.33 tokens per second)\n",
            "llama_print_timings:        eval time =   24501.18 ms /   341 runs   (   71.85 ms per token,    13.92 tokens per second)\n",
            "llama_print_timings:       total time =   26338.25 ms /   358 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =      14.68 ms /   346 runs   (    0.04 ms per token, 23571.09 tokens per second)\n",
            "llama_print_timings: prompt eval time =     432.20 ms /    15 tokens (   28.81 ms per token,    34.71 tokens per second)\n",
            "llama_print_timings:        eval time =   25231.96 ms /   345 runs   (   73.14 ms per token,    13.67 tokens per second)\n",
            "llama_print_timings:       total time =   27055.24 ms /   360 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =      17.72 ms /   418 runs   (    0.04 ms per token, 23585.17 tokens per second)\n",
            "llama_print_timings: prompt eval time =     525.44 ms /    18 tokens (   29.19 ms per token,    34.26 tokens per second)\n",
            "llama_print_timings:        eval time =   30332.97 ms /   417 runs   (   72.74 ms per token,    13.75 tokens per second)\n",
            "llama_print_timings:       total time =   32519.65 ms /   435 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       5.75 ms /   137 runs   (    0.04 ms per token, 23830.23 tokens per second)\n",
            "llama_print_timings: prompt eval time =     437.10 ms /    15 tokens (   29.14 ms per token,    34.32 tokens per second)\n",
            "llama_print_timings:        eval time =    9907.01 ms /   136 runs   (   72.85 ms per token,    13.73 tokens per second)\n",
            "llama_print_timings:       total time =   10840.15 ms /   151 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       6.96 ms /   167 runs   (    0.04 ms per token, 23980.47 tokens per second)\n",
            "llama_print_timings: prompt eval time =     529.72 ms /    17 tokens (   31.16 ms per token,    32.09 tokens per second)\n",
            "llama_print_timings:        eval time =   11980.74 ms /   166 runs   (   72.17 ms per token,    13.86 tokens per second)\n",
            "llama_print_timings:       total time =   13155.81 ms /   183 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =      16.05 ms /   381 runs   (    0.04 ms per token, 23738.32 tokens per second)\n",
            "llama_print_timings: prompt eval time =     422.98 ms /    16 tokens (   26.44 ms per token,    37.83 tokens per second)\n",
            "llama_print_timings:        eval time =   27028.94 ms /   380 runs   (   71.13 ms per token,    14.06 tokens per second)\n",
            "llama_print_timings:       total time =   28963.82 ms /   396 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =      14.69 ms /   347 runs   (    0.04 ms per token, 23627.94 tokens per second)\n",
            "llama_print_timings: prompt eval time =     434.87 ms /    15 tokens (   28.99 ms per token,    34.49 tokens per second)\n",
            "llama_print_timings:        eval time =   24830.62 ms /   346 runs   (   71.76 ms per token,    13.93 tokens per second)\n",
            "llama_print_timings:       total time =   26594.69 ms /   361 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =      14.73 ms /   345 runs   (    0.04 ms per token, 23413.64 tokens per second)\n",
            "llama_print_timings: prompt eval time =     345.21 ms /    12 tokens (   28.77 ms per token,    34.76 tokens per second)\n",
            "llama_print_timings:        eval time =   25264.82 ms /   344 runs   (   73.44 ms per token,    13.62 tokens per second)\n",
            "llama_print_timings:       total time =   26948.44 ms /   356 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       5.67 ms /   132 runs   (    0.04 ms per token, 23268.11 tokens per second)\n",
            "llama_print_timings: prompt eval time =     413.44 ms /    15 tokens (   27.56 ms per token,    36.28 tokens per second)\n",
            "llama_print_timings:        eval time =    8829.31 ms /   131 runs   (   67.40 ms per token,    14.84 tokens per second)\n",
            "llama_print_timings:       total time =    9732.27 ms /   146 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       5.38 ms /   129 runs   (    0.04 ms per token, 23959.88 tokens per second)\n",
            "llama_print_timings: prompt eval time =     445.70 ms /    16 tokens (   27.86 ms per token,    35.90 tokens per second)\n",
            "llama_print_timings:        eval time =    9356.65 ms /   128 runs   (   73.10 ms per token,    13.68 tokens per second)\n",
            "llama_print_timings:       total time =   10256.08 ms /   144 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =      13.97 ms /   332 runs   (    0.04 ms per token, 23760.11 tokens per second)\n",
            "llama_print_timings: prompt eval time =     426.01 ms /    16 tokens (   26.63 ms per token,    37.56 tokens per second)\n",
            "llama_print_timings:        eval time =   24385.11 ms /   331 runs   (   73.67 ms per token,    13.57 tokens per second)\n",
            "llama_print_timings:       total time =   26152.53 ms /   347 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       5.03 ms /   117 runs   (    0.04 ms per token, 23265.06 tokens per second)\n",
            "llama_print_timings: prompt eval time =     436.12 ms /    14 tokens (   31.15 ms per token,    32.10 tokens per second)\n",
            "llama_print_timings:        eval time =    8598.18 ms /   116 runs   (   74.12 ms per token,    13.49 tokens per second)\n",
            "llama_print_timings:       total time =    9484.06 ms /   130 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       5.43 ms /   129 runs   (    0.04 ms per token, 23739.42 tokens per second)\n",
            "llama_print_timings: prompt eval time =     534.17 ms /    17 tokens (   31.42 ms per token,    31.83 tokens per second)\n",
            "llama_print_timings:        eval time =    9409.99 ms /   128 runs   (   73.52 ms per token,    13.60 tokens per second)\n",
            "llama_print_timings:       total time =   10453.01 ms /   145 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =      14.47 ms /   342 runs   (    0.04 ms per token, 23636.74 tokens per second)\n",
            "llama_print_timings: prompt eval time =     434.55 ms /    16 tokens (   27.16 ms per token,    36.82 tokens per second)\n",
            "llama_print_timings:        eval time =   24318.67 ms /   341 runs   (   71.32 ms per token,    14.02 tokens per second)\n",
            "llama_print_timings:       total time =   26092.18 ms /   357 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =      14.47 ms /   342 runs   (    0.04 ms per token, 23640.01 tokens per second)\n",
            "llama_print_timings: prompt eval time =     537.46 ms /    17 tokens (   31.62 ms per token,    31.63 tokens per second)\n",
            "llama_print_timings:        eval time =   24597.98 ms /   341 runs   (   72.13 ms per token,    13.86 tokens per second)\n",
            "llama_print_timings:       total time =   26463.97 ms /   358 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =      18.77 ms /   445 runs   (    0.04 ms per token, 23704.26 tokens per second)\n",
            "llama_print_timings: prompt eval time =     537.00 ms /    17 tokens (   31.59 ms per token,    31.66 tokens per second)\n",
            "llama_print_timings:        eval time =   32247.44 ms /   444 runs   (   72.63 ms per token,    13.77 tokens per second)\n",
            "llama_print_timings:       total time =   34593.53 ms /   461 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =      18.76 ms /   445 runs   (    0.04 ms per token, 23715.63 tokens per second)\n",
            "llama_print_timings: prompt eval time =     517.26 ms /    17 tokens (   30.43 ms per token,    32.87 tokens per second)\n",
            "llama_print_timings:        eval time =   32271.67 ms /   444 runs   (   72.68 ms per token,    13.76 tokens per second)\n",
            "llama_print_timings:       total time =   34539.66 ms /   461 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       5.67 ms /   135 runs   (    0.04 ms per token, 23788.55 tokens per second)\n",
            "llama_print_timings: prompt eval time =     321.70 ms /    12 tokens (   26.81 ms per token,    37.30 tokens per second)\n",
            "llama_print_timings:        eval time =    9791.73 ms /   134 runs   (   73.07 ms per token,    13.69 tokens per second)\n",
            "llama_print_timings:       total time =   10609.25 ms /   146 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       5.40 ms /   128 runs   (    0.04 ms per token, 23725.67 tokens per second)\n",
            "llama_print_timings: prompt eval time =     416.65 ms /    16 tokens (   26.04 ms per token,    38.40 tokens per second)\n",
            "llama_print_timings:        eval time =    8606.50 ms /   127 runs   (   67.77 ms per token,    14.76 tokens per second)\n",
            "llama_print_timings:       total time =    9465.24 ms /   143 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =      15.14 ms /   360 runs   (    0.04 ms per token, 23778.07 tokens per second)\n",
            "llama_print_timings: prompt eval time =     416.91 ms /    14 tokens (   29.78 ms per token,    33.58 tokens per second)\n",
            "llama_print_timings:        eval time =   27140.73 ms /   359 runs   (   75.60 ms per token,    13.23 tokens per second)\n",
            "llama_print_timings:       total time =   28987.69 ms /   373 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =      16.38 ms /   388 runs   (    0.04 ms per token, 23685.98 tokens per second)\n",
            "llama_print_timings: prompt eval time =     526.72 ms /    18 tokens (   29.26 ms per token,    34.17 tokens per second)\n",
            "llama_print_timings:        eval time =   27812.94 ms /   387 runs   (   71.87 ms per token,    13.91 tokens per second)\n",
            "llama_print_timings:       total time =   29869.12 ms /   405 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =      13.92 ms /   332 runs   (    0.04 ms per token, 23850.57 tokens per second)\n",
            "llama_print_timings: prompt eval time =     427.19 ms /    16 tokens (   26.70 ms per token,    37.45 tokens per second)\n",
            "llama_print_timings:        eval time =   23699.05 ms /   331 runs   (   71.60 ms per token,    13.97 tokens per second)\n",
            "llama_print_timings:       total time =   25428.25 ms /   347 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =      14.52 ms /   342 runs   (    0.04 ms per token, 23561.83 tokens per second)\n",
            "llama_print_timings: prompt eval time =     531.52 ms /    18 tokens (   29.53 ms per token,    33.87 tokens per second)\n",
            "llama_print_timings:        eval time =   24233.17 ms /   341 runs   (   71.07 ms per token,    14.07 tokens per second)\n",
            "llama_print_timings:       total time =   26057.55 ms /   359 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       4.91 ms /   117 runs   (    0.04 ms per token, 23809.52 tokens per second)\n",
            "llama_print_timings: prompt eval time =     428.65 ms /    14 tokens (   30.62 ms per token,    32.66 tokens per second)\n",
            "llama_print_timings:        eval time =    8429.49 ms /   116 runs   (   72.67 ms per token,    13.76 tokens per second)\n",
            "llama_print_timings:       total time =    9301.35 ms /   130 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =      14.47 ms /   338 runs   (    0.04 ms per token, 23365.13 tokens per second)\n",
            "llama_print_timings: prompt eval time =     429.70 ms /    16 tokens (   26.86 ms per token,    37.24 tokens per second)\n",
            "llama_print_timings:        eval time =   24294.08 ms /   337 runs   (   72.09 ms per token,    13.87 tokens per second)\n",
            "llama_print_timings:       total time =   26003.17 ms /   353 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       5.84 ms /   137 runs   (    0.04 ms per token, 23462.92 tokens per second)\n",
            "llama_print_timings: prompt eval time =     514.21 ms /    17 tokens (   30.25 ms per token,    33.06 tokens per second)\n",
            "llama_print_timings:        eval time =    9911.30 ms /   136 runs   (   72.88 ms per token,    13.72 tokens per second)\n",
            "llama_print_timings:       total time =   10950.01 ms /   153 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =      15.27 ms /   362 runs   (    0.04 ms per token, 23709.72 tokens per second)\n",
            "llama_print_timings: prompt eval time =     429.18 ms /    15 tokens (   28.61 ms per token,    34.95 tokens per second)\n",
            "llama_print_timings:        eval time =   25943.77 ms /   361 runs   (   71.87 ms per token,    13.91 tokens per second)\n",
            "llama_print_timings:       total time =   27856.55 ms /   376 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =      14.56 ms /   342 runs   (    0.04 ms per token, 23487.40 tokens per second)\n",
            "llama_print_timings: prompt eval time =     428.67 ms /    16 tokens (   26.79 ms per token,    37.32 tokens per second)\n",
            "llama_print_timings:        eval time =   25177.89 ms /   341 runs   (   73.84 ms per token,    13.54 tokens per second)\n",
            "llama_print_timings:       total time =   26957.52 ms /   357 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       5.17 ms /   124 runs   (    0.04 ms per token, 23970.62 tokens per second)\n",
            "llama_print_timings: prompt eval time =     441.14 ms /    13 tokens (   33.93 ms per token,    29.47 tokens per second)\n",
            "llama_print_timings:        eval time =    8749.91 ms /   123 runs   (   71.14 ms per token,    14.06 tokens per second)\n",
            "llama_print_timings:       total time =    9644.03 ms /   136 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =      14.01 ms /   332 runs   (    0.04 ms per token, 23690.60 tokens per second)\n",
            "llama_print_timings: prompt eval time =     439.46 ms /    16 tokens (   27.47 ms per token,    36.41 tokens per second)\n",
            "llama_print_timings:        eval time =   23479.96 ms /   331 runs   (   70.94 ms per token,    14.10 tokens per second)\n",
            "llama_print_timings:       total time =   25250.42 ms /   347 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =      16.91 ms /   399 runs   (    0.04 ms per token, 23599.69 tokens per second)\n",
            "llama_print_timings: prompt eval time =     430.57 ms /    15 tokens (   28.70 ms per token,    34.84 tokens per second)\n",
            "llama_print_timings:        eval time =   29457.90 ms /   398 runs   (   74.01 ms per token,    13.51 tokens per second)\n",
            "llama_print_timings:       total time =   31466.82 ms /   413 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =      14.27 ms /   337 runs   (    0.04 ms per token, 23620.94 tokens per second)\n",
            "llama_print_timings: prompt eval time =     424.55 ms /    15 tokens (   28.30 ms per token,    35.33 tokens per second)\n",
            "llama_print_timings:        eval time =   24262.19 ms /   336 runs   (   72.21 ms per token,    13.85 tokens per second)\n",
            "llama_print_timings:       total time =   26027.91 ms /   351 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =      15.53 ms /   371 runs   (    0.04 ms per token, 23887.71 tokens per second)\n",
            "llama_print_timings: prompt eval time =     427.84 ms /    16 tokens (   26.74 ms per token,    37.40 tokens per second)\n",
            "llama_print_timings:        eval time =   26665.33 ms /   370 runs   (   72.07 ms per token,    13.88 tokens per second)\n",
            "llama_print_timings:       total time =   28461.70 ms /   386 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =      14.68 ms /   347 runs   (    0.04 ms per token, 23635.99 tokens per second)\n",
            "llama_print_timings: prompt eval time =     420.83 ms /    15 tokens (   28.06 ms per token,    35.64 tokens per second)\n",
            "llama_print_timings:        eval time =   24910.50 ms /   346 runs   (   72.00 ms per token,    13.89 tokens per second)\n",
            "llama_print_timings:       total time =   26707.58 ms /   361 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       7.05 ms /   167 runs   (    0.04 ms per token, 23674.51 tokens per second)\n",
            "llama_print_timings: prompt eval time =     524.15 ms /    17 tokens (   30.83 ms per token,    32.43 tokens per second)\n",
            "llama_print_timings:        eval time =   11970.99 ms /   166 runs   (   72.11 ms per token,    13.87 tokens per second)\n",
            "llama_print_timings:       total time =   13140.97 ms /   183 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       5.01 ms /   120 runs   (    0.04 ms per token, 23937.76 tokens per second)\n",
            "llama_print_timings: prompt eval time =     459.37 ms /    16 tokens (   28.71 ms per token,    34.83 tokens per second)\n",
            "llama_print_timings:        eval time =    8683.45 ms /   119 runs   (   72.97 ms per token,    13.70 tokens per second)\n",
            "llama_print_timings:       total time =    9566.40 ms /   135 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =      14.39 ms /   342 runs   (    0.04 ms per token, 23769.81 tokens per second)\n",
            "llama_print_timings: prompt eval time =     447.59 ms /    16 tokens (   27.97 ms per token,    35.75 tokens per second)\n",
            "llama_print_timings:        eval time =   25382.06 ms /   341 runs   (   74.43 ms per token,    13.43 tokens per second)\n",
            "llama_print_timings:       total time =   27205.22 ms /   357 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =      14.82 ms /   347 runs   (    0.04 ms per token, 23419.05 tokens per second)\n",
            "llama_print_timings: prompt eval time =     426.37 ms /    15 tokens (   28.42 ms per token,    35.18 tokens per second)\n",
            "llama_print_timings:        eval time =   24920.99 ms /   346 runs   (   72.03 ms per token,    13.88 tokens per second)\n",
            "llama_print_timings:       total time =   26803.30 ms /   361 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =      14.87 ms /   351 runs   (    0.04 ms per token, 23604.57 tokens per second)\n",
            "llama_print_timings: prompt eval time =     331.53 ms /    12 tokens (   27.63 ms per token,    36.20 tokens per second)\n",
            "llama_print_timings:        eval time =   25150.13 ms /   350 runs   (   71.86 ms per token,    13.92 tokens per second)\n",
            "llama_print_timings:       total time =   26840.21 ms /   362 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       5.46 ms /   127 runs   (    0.04 ms per token, 23247.30 tokens per second)\n",
            "llama_print_timings: prompt eval time =     523.49 ms /    18 tokens (   29.08 ms per token,    34.38 tokens per second)\n",
            "llama_print_timings:        eval time =    8744.67 ms /   126 runs   (   69.40 ms per token,    14.41 tokens per second)\n",
            "llama_print_timings:       total time =    9744.64 ms /   144 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =      15.23 ms /   360 runs   (    0.04 ms per token, 23643.77 tokens per second)\n",
            "llama_print_timings: prompt eval time =     437.21 ms /    14 tokens (   31.23 ms per token,    32.02 tokens per second)\n",
            "llama_print_timings:        eval time =   26052.45 ms /   359 runs   (   72.57 ms per token,    13.78 tokens per second)\n",
            "llama_print_timings:       total time =   27821.39 ms /   373 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       5.76 ms /   133 runs   (    0.04 ms per token, 23086.27 tokens per second)\n",
            "llama_print_timings: prompt eval time =     540.90 ms /    18 tokens (   30.05 ms per token,    33.28 tokens per second)\n",
            "llama_print_timings:        eval time =    9601.73 ms /   132 runs   (   72.74 ms per token,    13.75 tokens per second)\n",
            "llama_print_timings:       total time =   10641.69 ms /   150 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     518.97 ms\n",
            "llama_print_timings:      sample time =       6.96 ms /   161 runs   (    0.04 ms per token, 23148.81 tokens per second)\n",
            "llama_print_timings: prompt eval time =     513.19 ms /    20 tokens (   25.66 ms per token,    38.97 tokens per second)\n",
            "llama_print_timings:        eval time =   11661.87 ms /   160 runs   (   72.89 ms per token,    13.72 tokens per second)\n",
            "llama_print_timings:       total time =   12766.66 ms /   180 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i = 1\n",
        "print(data_4.loc[i, 'ReviewText'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCn25HcL6XF7",
        "outputId": "269a727a-1f19-4c7c-e2e6-b6467af46b8e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review #2: A page-turner with a powerful message.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_4.loc[i, 'model_response'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mtasc6Xs6da0",
        "outputId": "b988c910-0f20-4c29-884c-739576593925"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure, I'd be happy to help you analyze the review! Here's the JSON object with my predictions:\n",
            "\n",
            "{\n",
            "    \"Overall Sentiment\": \"Positive\",\n",
            "    \"Writing Style\": \"Positive\",\n",
            "    \"Emotional Impact\": \"Positive\",\n",
            "    \"Pacing\": [\"fast-paced\"],\n",
            "    \"Ending\": [\"satisfying\"],\n",
            "    \"Response\": \"Thank you for your positive review! We're glad you enjoyed the book and found it to be a page-turner with a powerful message. We hope you'll continue to read more of our books in the future.\"\n",
            "}\n",
            "\n",
            "Here's how I arrived at these predictions:\n",
            "\n",
            "1. Overall Sentiment: The review expresses a positive sentiment towards the book, so I classified it as \"Positive\".\n",
            "2. Writing Style: The reviewer mentions that the book is a \"page-turner\", which suggests that the writing style is engaging and fast-paced. Therefore, I classified the aspect of \"Writing Style\" as \"Positive\".\n",
            "3. Emotional Impact: The reviewer mentions that the book has a \"powerful message\", which suggests that it had an emotional impact on them. Therefore, I classified the aspect of \"Emotional Impact\" as \"Positive\".\n",
            "4. Pacing: The reviewer mentions that the book is \"fast-paced\", which suggests that they enjoyed the quick pace of the story. Therefore, I classified the aspect of \"Pacing\" as \"Liked\".\n",
            "5. Ending: The reviewer mentions that the ending is \"satisfying\", which suggests that they found the conclusion of the story to be satisfying and well-resolved. Therefore, I classified the aspect of \"Ending\" as \"Liked\".\n",
            "6. Response: Based on the positive sentiment of the review, I crafted a response that is polite, empathetic, and thankful for the reviewer's positive feedback.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# applying the function to the model response\n",
        "data_4['model_response_parsed'] = data_4['model_response'].apply(extract_json_data)\n",
        "data_4['model_response_parsed'].head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "262Pj92VEgdU",
        "outputId": "5286a29d-1803-42d9-8ff3-da95fb1f6405"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    {'Overall Sentiment': 'Negative', 'Writing Sty...\n",
              "1    {'Overall Sentiment': 'Positive', 'Writing Sty...\n",
              "2    {'Overall Sentiment': 'Positive', 'Writing Sty...\n",
              "3    {'Overall Sentiment': 'Positive', 'Writing Sty...\n",
              "4    {'Overall Sentiment': 'Neutral', 'Writing Styl...\n",
              "Name: model_response_parsed, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model_response_parsed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>{'Overall Sentiment': 'Negative', 'Writing Sty...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>{'Overall Sentiment': 'Positive', 'Writing Sty...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>{'Overall Sentiment': 'Positive', 'Writing Sty...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>{'Overall Sentiment': 'Positive', 'Writing Sty...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>{'Overall Sentiment': 'Neutral', 'Writing Styl...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter rows where the parsed JSON result is an empty dictionary\n",
        "data_4[data_4.model_response_parsed == {}]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "B7hm3nROEbMu",
        "outputId": "a6986d74-299f-4629-faae-a469f2975114"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [ReviewText, Sentiment, model_response, model_response_parsed]\n",
              "Index: []"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-80f1bb6e-975f-48e0-a815-397d2568811f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ReviewText</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>model_response</th>\n",
              "      <th>model_response_parsed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-80f1bb6e-975f-48e0-a815-397d2568811f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-80f1bb6e-975f-48e0-a815-397d2568811f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-80f1bb6e-975f-48e0-a815-397d2568811f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "repr_error": "Out of range float values are not JSON compliant: nan"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Each key in the JSON becomes a column, and each value becomes a row cell.\n",
        "model_response_parsed_df_4 = pd.json_normalize(data_4['model_response_parsed'])\n",
        "model_response_parsed_df_4.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "v21yWILmEpSs",
        "outputId": "ebba15c1-16c2-42bf-c512-ca34a74c783d"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  Overall Sentiment Writing Style Emotional Impact            Pacing  \\\n",
              "0          Negative      Negative         Negative        [disliked]   \n",
              "1          Positive      Positive         Positive      [fast-paced]   \n",
              "2          Positive      Positive         Positive           [liked]   \n",
              "3          Positive      Positive         Positive           [liked]   \n",
              "4           Neutral       Neutral          Neutral  [Not Applicable]   \n",
              "\n",
              "             Ending                                           Response  \n",
              "0        [disliked]  Sorry to hear that you didn't enjoy the book. ...  \n",
              "1      [satisfying]  Thank you for your positive review! We're glad...  \n",
              "2           [liked]  Thank you so much for your positive review! We...  \n",
              "3           [liked]  Thank you so much for your review! We're glad ...  \n",
              "4  [Not Applicable]  Thank you for your review! We're glad you foun...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-99521a5b-f9bf-4156-bf9b-919f0f4e50ec\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Overall Sentiment</th>\n",
              "      <th>Writing Style</th>\n",
              "      <th>Emotional Impact</th>\n",
              "      <th>Pacing</th>\n",
              "      <th>Ending</th>\n",
              "      <th>Response</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Negative</td>\n",
              "      <td>Negative</td>\n",
              "      <td>Negative</td>\n",
              "      <td>[disliked]</td>\n",
              "      <td>[disliked]</td>\n",
              "      <td>Sorry to hear that you didn't enjoy the book. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Positive</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Positive</td>\n",
              "      <td>[fast-paced]</td>\n",
              "      <td>[satisfying]</td>\n",
              "      <td>Thank you for your positive review! We're glad...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Positive</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Positive</td>\n",
              "      <td>[liked]</td>\n",
              "      <td>[liked]</td>\n",
              "      <td>Thank you so much for your positive review! We...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Positive</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Positive</td>\n",
              "      <td>[liked]</td>\n",
              "      <td>[liked]</td>\n",
              "      <td>Thank you so much for your review! We're glad ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Neutral</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>[Not Applicable]</td>\n",
              "      <td>[Not Applicable]</td>\n",
              "      <td>Thank you for your review! We're glad you foun...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-99521a5b-f9bf-4156-bf9b-919f0f4e50ec')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-99521a5b-f9bf-4156-bf9b-919f0f4e50ec button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-99521a5b-f9bf-4156-bf9b-919f0f4e50ec');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-83c777cf-6b3a-4b40-8575-9b11b3b55c20\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-83c777cf-6b3a-4b40-8575-9b11b3b55c20')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-83c777cf-6b3a-4b40-8575-9b11b3b55c20 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "model_response_parsed_df_4",
              "summary": "{\n  \"name\": \"model_response_parsed_df_4\",\n  \"rows\": 100,\n  \"fields\": [\n    {\n      \"column\": \"Overall Sentiment\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Negative\",\n          \"Positive\",\n          \"Neutral\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Writing Style\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Positive\",\n          \"Decent\",\n          \"Negative\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Emotional Impact\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Positive\",\n          \"Mediocre\",\n          \"Neutral\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Pacing\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Ending\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Response\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 53,\n        \"samples\": [\n          \"Thank you for your positive review! We're glad you enjoyed the book and found the writing style and emotional impact to be captivating. If you have any suggestions for improvement, we would love to hear them.\",\n          \"Thank you for taking the time to share your feedback. Sorry to hear that you found the book to be a waste of time. We appreciate your input and will review it to see how we can improve our selection.\",\n          \"Thank you for your review! We appreciate your feedback and would love to hear more about what you would have liked to see in the book's pacing and ending.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Merge\n",
        "data_with_parsed_model_output_4 = pd.concat([data_4, model_response_parsed_df_4], axis=1)\n",
        "data_with_parsed_model_output_4.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        },
        "id": "uqjAltWGEw27",
        "outputId": "e1ddee63-3fa2-477d-dd40-d73f03354908"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                          ReviewText  Sentiment  \\\n",
              "0              Review #1: Highly disappointing read.          0   \n",
              "1  Review #2: A page-turner with a powerful message.          2   \n",
              "2          Review #3: A masterpiece of storytelling.          2   \n",
              "3        Review #4: Heartwarming and inspiring read.          2   \n",
              "4        Review #5: Neither good nor bad, just fine.          1   \n",
              "5             Review #6: Absolutely loved this book!          2   \n",
              "6            Review #7: Overhyped and underwhelming.          0   \n",
              "7  Review #8: Some parts were slow, overall reada...          1   \n",
              "8    Review #9: Had its moments, but not remarkable.          1   \n",
              "9  Review #10: Neutral feelings, not very impactful.          1   \n",
              "\n",
              "                                      model_response  \\\n",
              "0  Sure, I'd be happy to help you analyze the rev...   \n",
              "1  Sure, I'd be happy to help you analyze the rev...   \n",
              "2  Sure, I'd be happy to help you analyze the rev...   \n",
              "3  Sure, I'd be happy to help you analyze the rev...   \n",
              "4  Sure, I'd be happy to help you analyze this bo...   \n",
              "5  Sure, I'd be happy to help you with that! Here...   \n",
              "6  Sure, I'd be happy to help you analyze this bo...   \n",
              "7  Sure, I'd be happy to help you analyze this bo...   \n",
              "8  Sure, I'd be happy to help you analyze this bo...   \n",
              "9  Sure, I'd be happy to help! Here's the analysi...   \n",
              "\n",
              "                               model_response_parsed Overall Sentiment  \\\n",
              "0  {'Overall Sentiment': 'Negative', 'Writing Sty...          Negative   \n",
              "1  {'Overall Sentiment': 'Positive', 'Writing Sty...          Positive   \n",
              "2  {'Overall Sentiment': 'Positive', 'Writing Sty...          Positive   \n",
              "3  {'Overall Sentiment': 'Positive', 'Writing Sty...          Positive   \n",
              "4  {'Overall Sentiment': 'Neutral', 'Writing Styl...           Neutral   \n",
              "5  {'Overall Sentiment': 'Positive', 'Writing Sty...          Positive   \n",
              "6  {'Overall Sentiment': 'Negative', 'Writing Sty...          Negative   \n",
              "7  {'Overall Sentiment': 'Neutral', 'Writing Styl...           Neutral   \n",
              "8  {'Overall Sentiment': 'Neutral', 'Writing Styl...           Neutral   \n",
              "9  {'Overall Sentiment': 'Neutral', 'Writing Styl...           Neutral   \n",
              "\n",
              "  Writing Style Emotional Impact            Pacing            Ending  \\\n",
              "0      Negative         Negative        [disliked]        [disliked]   \n",
              "1      Positive         Positive      [fast-paced]      [satisfying]   \n",
              "2      Positive         Positive           [liked]           [liked]   \n",
              "3      Positive         Positive           [liked]           [liked]   \n",
              "4       Neutral          Neutral  [Not Applicable]  [Not Applicable]   \n",
              "5      Positive         Positive           [Liked]           [Liked]   \n",
              "6      Negative         Negative   [underwhelming]        [disliked]   \n",
              "7       Neutral          Neutral            [slow]  [not applicable]   \n",
              "8       Neutral          Neutral  [Not Applicable]  [Not Applicable]   \n",
              "9       Neutral          Neutral  [Not Applicable]  [Not Applicable]   \n",
              "\n",
              "                                            Response  \n",
              "0  Sorry to hear that you didn't enjoy the book. ...  \n",
              "1  Thank you for your positive review! We're glad...  \n",
              "2  Thank you so much for your positive review! We...  \n",
              "3  Thank you so much for your review! We're glad ...  \n",
              "4  Thank you for your review! We're glad you foun...  \n",
              "5  Thank you so much for your review! We're glad ...  \n",
              "6  Thank you for sharing your feedback. Sorry to ...  \n",
              "7  Thank you for your review! We're glad you foun...  \n",
              "8  Thank you for taking the time to share your th...  \n",
              "9  Thank you for taking the time to share your th...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-666e6347-74a9-4202-b8fb-1de40326c825\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ReviewText</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>model_response</th>\n",
              "      <th>model_response_parsed</th>\n",
              "      <th>Overall Sentiment</th>\n",
              "      <th>Writing Style</th>\n",
              "      <th>Emotional Impact</th>\n",
              "      <th>Pacing</th>\n",
              "      <th>Ending</th>\n",
              "      <th>Response</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Review #1: Highly disappointing read.</td>\n",
              "      <td>0</td>\n",
              "      <td>Sure, I'd be happy to help you analyze the rev...</td>\n",
              "      <td>{'Overall Sentiment': 'Negative', 'Writing Sty...</td>\n",
              "      <td>Negative</td>\n",
              "      <td>Negative</td>\n",
              "      <td>Negative</td>\n",
              "      <td>[disliked]</td>\n",
              "      <td>[disliked]</td>\n",
              "      <td>Sorry to hear that you didn't enjoy the book. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Review #2: A page-turner with a powerful message.</td>\n",
              "      <td>2</td>\n",
              "      <td>Sure, I'd be happy to help you analyze the rev...</td>\n",
              "      <td>{'Overall Sentiment': 'Positive', 'Writing Sty...</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Positive</td>\n",
              "      <td>[fast-paced]</td>\n",
              "      <td>[satisfying]</td>\n",
              "      <td>Thank you for your positive review! We're glad...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Review #3: A masterpiece of storytelling.</td>\n",
              "      <td>2</td>\n",
              "      <td>Sure, I'd be happy to help you analyze the rev...</td>\n",
              "      <td>{'Overall Sentiment': 'Positive', 'Writing Sty...</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Positive</td>\n",
              "      <td>[liked]</td>\n",
              "      <td>[liked]</td>\n",
              "      <td>Thank you so much for your positive review! We...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Review #4: Heartwarming and inspiring read.</td>\n",
              "      <td>2</td>\n",
              "      <td>Sure, I'd be happy to help you analyze the rev...</td>\n",
              "      <td>{'Overall Sentiment': 'Positive', 'Writing Sty...</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Positive</td>\n",
              "      <td>[liked]</td>\n",
              "      <td>[liked]</td>\n",
              "      <td>Thank you so much for your review! We're glad ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Review #5: Neither good nor bad, just fine.</td>\n",
              "      <td>1</td>\n",
              "      <td>Sure, I'd be happy to help you analyze this bo...</td>\n",
              "      <td>{'Overall Sentiment': 'Neutral', 'Writing Styl...</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>[Not Applicable]</td>\n",
              "      <td>[Not Applicable]</td>\n",
              "      <td>Thank you for your review! We're glad you foun...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Review #6: Absolutely loved this book!</td>\n",
              "      <td>2</td>\n",
              "      <td>Sure, I'd be happy to help you with that! Here...</td>\n",
              "      <td>{'Overall Sentiment': 'Positive', 'Writing Sty...</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Positive</td>\n",
              "      <td>[Liked]</td>\n",
              "      <td>[Liked]</td>\n",
              "      <td>Thank you so much for your review! We're glad ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Review #7: Overhyped and underwhelming.</td>\n",
              "      <td>0</td>\n",
              "      <td>Sure, I'd be happy to help you analyze this bo...</td>\n",
              "      <td>{'Overall Sentiment': 'Negative', 'Writing Sty...</td>\n",
              "      <td>Negative</td>\n",
              "      <td>Negative</td>\n",
              "      <td>Negative</td>\n",
              "      <td>[underwhelming]</td>\n",
              "      <td>[disliked]</td>\n",
              "      <td>Thank you for sharing your feedback. Sorry to ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Review #8: Some parts were slow, overall reada...</td>\n",
              "      <td>1</td>\n",
              "      <td>Sure, I'd be happy to help you analyze this bo...</td>\n",
              "      <td>{'Overall Sentiment': 'Neutral', 'Writing Styl...</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>[slow]</td>\n",
              "      <td>[not applicable]</td>\n",
              "      <td>Thank you for your review! We're glad you foun...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Review #9: Had its moments, but not remarkable.</td>\n",
              "      <td>1</td>\n",
              "      <td>Sure, I'd be happy to help you analyze this bo...</td>\n",
              "      <td>{'Overall Sentiment': 'Neutral', 'Writing Styl...</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>[Not Applicable]</td>\n",
              "      <td>[Not Applicable]</td>\n",
              "      <td>Thank you for taking the time to share your th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Review #10: Neutral feelings, not very impactful.</td>\n",
              "      <td>1</td>\n",
              "      <td>Sure, I'd be happy to help! Here's the analysi...</td>\n",
              "      <td>{'Overall Sentiment': 'Neutral', 'Writing Styl...</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>[Not Applicable]</td>\n",
              "      <td>[Not Applicable]</td>\n",
              "      <td>Thank you for taking the time to share your th...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-666e6347-74a9-4202-b8fb-1de40326c825')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-666e6347-74a9-4202-b8fb-1de40326c825 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-666e6347-74a9-4202-b8fb-1de40326c825');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-84e2e1d0-9a57-47cb-9ae7-be9aab70b537\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-84e2e1d0-9a57-47cb-9ae7-be9aab70b537')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-84e2e1d0-9a57-47cb-9ae7-be9aab70b537 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data_with_parsed_model_output_4",
              "summary": "{\n  \"name\": \"data_with_parsed_model_output_4\",\n  \"rows\": 100,\n  \"fields\": [\n    {\n      \"column\": \"ReviewText\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"Review #84: Uninspiring and boring.\",\n          \"Review #54: The best book I've read this year.\",\n          \"Review #71: Neither good nor bad, just fine.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sentiment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0,\n          2,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"model_response\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 66,\n        \"samples\": [\n          \"Sure, I'd be happy to help you analyze this book review! Here's the information you requested:\\n\\n{\\n\\\"Overall Sentiment\\\": \\\"Neutral\\\",\\n\\\"Writing Style\\\": \\\"Neutral\\\",\\n\\\"Emotional Impact\\\": \\\"Neutral\\\",\\n\\\"Pacing\\\": [\\\"Not Applicable\\\"],\\n\\\"Ending\\\": [\\\"Not Applicable\\\"],\\n\\\"Response\\\": \\\"Thank you for your review! We're sorry to hear that the book wasn't too exciting for you, but we appreciate your feedback. If you have any suggestions on how we could improve, please don't hesitate to let us know.\\\"\\n}\\n\\nHere's how I arrived at these answers:\\n\\n1. Overall Sentiment: The review states that the book was \\\"okay, not too exciting,\\\" which suggests a neutral sentiment.\\n2. Writing Style: The review does not mention the writing style, so I have classified it as \\\"Not Applicable.\\\"\\n3. Emotional Impact: The review does not mention any emotional impact, so I have classified it as \\\"Not Applicable.\\\"\\n4. Pacing: The review does not mention the pacing, so I have classified it as \\\"Not Applicable.\\\"\\n5. Ending: The review does not mention the ending, so I have classified it as \\\"Not Applicable.\\\"\\n6. Response: Based on the neutral sentiment of the review, I have crafted a response that is polite, empathetic, and open to feedback.\",\n          \"Sure, I'd be happy to help you analyze this book review! Here's the information you requested:\\n\\n{\\n\\\"Overall Sentiment\\\": \\\"Negative\\\",\\n\\\"Writing Style\\\": \\\"Negative\\\",\\n\\\"Emotional Impact\\\": \\\"Not Applicable\\\",\\n\\\"Pacing\\\": [\\\"disliked\\\"],\\n\\\"Ending\\\": [\\\"disliked\\\"],\\n\\\"Response\\\": \\\"Thank you for taking the time to share your feedback. Sorry to hear that you found the character development to be poor. We will review your feedback and use it to improve our future books.\\\"\\n}\\n\\nHere's how I arrived at these conclusions:\\n\\n1. Overall Sentiment: The review expresses disappointment with the book, so the overall sentiment is Negative.\\n2. Writing Style: The reviewer mentions \\\"poor character development,\\\" which suggests that the writing style did not effectively convey the characters or their development. Therefore, the sentiment for Writing Style is Negative.\\n3. Emotional Impact: The reviewer does not mention any emotional impact from the book, so the sentiment for Emotional Impact is Not Applicable.\\n4. Pacing: The reviewer mentions that the pacing was poor, so the sentiment for Pacing is Disliked.\\n5. Ending: The reviewer also mentions that the ending was disappointing, so the sentiment for Ending is Disliked.\\n6. Response: I would respond with a thank you and an apology for not meeting the reviewer's expectations. I would also ask for specific feedback on what could have improved their experience.\",\n          \"Sure, I'd be happy to help you analyze the review! Here's the JSON object with my predictions:\\n\\n{\\n    \\\"Overall Sentiment\\\": \\\"Negative\\\",\\n    \\\"Writing Style\\\": \\\"Negative\\\",\\n    \\\"Emotional Impact\\\": \\\"Negative\\\",\\n    \\\"Pacing\\\": [\\\"disliked\\\"],\\n    \\\"Ending\\\": [\\\"disliked\\\"],\\n    \\\"Response\\\": \\\"Sorry to hear that you didn't enjoy the book. We appreciate your feedback and will review it to see how we can improve.\\\"\\n}\\n\\nHere's how I arrived at these predictions:\\n\\n1. Overall Sentiment: The review expresses disappointment, so the overall sentiment is negative.\\n2. Writing Style: The reviewer mentions that the writing style is \\\"disappointing,\\\" so the sentiment for this aspect is negative.\\n3. Emotional Impact: The reviewer does not mention any emotional impact, so the sentiment for this aspect is neutral.\\n4. Pacing: The reviewer mentions that the pacing is \\\"disliked,\\\" so the sentiment for this aspect is negative.\\n5. Ending: The reviewer mentions that the ending is \\\"disliked,\\\" so the sentiment for this aspect is negative.\\n6. Response: I apologize for the negative experience and thank the reviewer for their feedback. I express a willingness to review the feedback to see how we can improve.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"model_response_parsed\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Overall Sentiment\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Negative\",\n          \"Positive\",\n          \"Neutral\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Writing Style\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Positive\",\n          \"Decent\",\n          \"Negative\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Emotional Impact\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Positive\",\n          \"Mediocre\",\n          \"Neutral\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Pacing\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Ending\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Response\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 53,\n        \"samples\": [\n          \"Thank you for your positive review! We're glad you enjoyed the book and found the writing style and emotional impact to be captivating. If you have any suggestions for improvement, we would love to hear them.\",\n          \"Thank you for taking the time to share your feedback. Sorry to hear that you found the book to be a waste of time. We appreciate your input and will review it to see how we can improve our selection.\",\n          \"Thank you for your review! We appreciate your feedback and would love to hear more about what you would have liked to see in the book's pacing and ending.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a final, cleaned dataset by removing raw model output columns\n",
        "final_data_4 = data_with_parsed_model_output_4.drop(['model_response','model_response_parsed'], axis=1)\n",
        "final_data_4.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "8dewD_2ZU1bZ",
        "outputId": "33110d8e-99f2-4a2a-f824-159e32dc6a7c"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                          ReviewText  Sentiment  \\\n",
              "0              Review #1: Highly disappointing read.          0   \n",
              "1  Review #2: A page-turner with a powerful message.          2   \n",
              "2          Review #3: A masterpiece of storytelling.          2   \n",
              "3        Review #4: Heartwarming and inspiring read.          2   \n",
              "4        Review #5: Neither good nor bad, just fine.          1   \n",
              "\n",
              "  Overall Sentiment Writing Style Emotional Impact            Pacing  \\\n",
              "0          Negative      Negative         Negative        [disliked]   \n",
              "1          Positive      Positive         Positive      [fast-paced]   \n",
              "2          Positive      Positive         Positive           [liked]   \n",
              "3          Positive      Positive         Positive           [liked]   \n",
              "4           Neutral       Neutral          Neutral  [Not Applicable]   \n",
              "\n",
              "             Ending                                           Response  \n",
              "0        [disliked]  Sorry to hear that you didn't enjoy the book. ...  \n",
              "1      [satisfying]  Thank you for your positive review! We're glad...  \n",
              "2           [liked]  Thank you so much for your positive review! We...  \n",
              "3           [liked]  Thank you so much for your review! We're glad ...  \n",
              "4  [Not Applicable]  Thank you for your review! We're glad you foun...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d17fc5c1-ddc6-46bb-8329-9b3ce7c68457\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ReviewText</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>Overall Sentiment</th>\n",
              "      <th>Writing Style</th>\n",
              "      <th>Emotional Impact</th>\n",
              "      <th>Pacing</th>\n",
              "      <th>Ending</th>\n",
              "      <th>Response</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Review #1: Highly disappointing read.</td>\n",
              "      <td>0</td>\n",
              "      <td>Negative</td>\n",
              "      <td>Negative</td>\n",
              "      <td>Negative</td>\n",
              "      <td>[disliked]</td>\n",
              "      <td>[disliked]</td>\n",
              "      <td>Sorry to hear that you didn't enjoy the book. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Review #2: A page-turner with a powerful message.</td>\n",
              "      <td>2</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Positive</td>\n",
              "      <td>[fast-paced]</td>\n",
              "      <td>[satisfying]</td>\n",
              "      <td>Thank you for your positive review! We're glad...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Review #3: A masterpiece of storytelling.</td>\n",
              "      <td>2</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Positive</td>\n",
              "      <td>[liked]</td>\n",
              "      <td>[liked]</td>\n",
              "      <td>Thank you so much for your positive review! We...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Review #4: Heartwarming and inspiring read.</td>\n",
              "      <td>2</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Positive</td>\n",
              "      <td>[liked]</td>\n",
              "      <td>[liked]</td>\n",
              "      <td>Thank you so much for your review! We're glad ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Review #5: Neither good nor bad, just fine.</td>\n",
              "      <td>1</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>[Not Applicable]</td>\n",
              "      <td>[Not Applicable]</td>\n",
              "      <td>Thank you for your review! We're glad you foun...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d17fc5c1-ddc6-46bb-8329-9b3ce7c68457')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d17fc5c1-ddc6-46bb-8329-9b3ce7c68457 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d17fc5c1-ddc6-46bb-8329-9b3ce7c68457');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-7298fe50-7ca9-48c8-a52b-8da77d8e1906\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7298fe50-7ca9-48c8-a52b-8da77d8e1906')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-7298fe50-7ca9-48c8-a52b-8da77d8e1906 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "final_data_4",
              "summary": "{\n  \"name\": \"final_data_4\",\n  \"rows\": 100,\n  \"fields\": [\n    {\n      \"column\": \"ReviewText\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"Review #84: Uninspiring and boring.\",\n          \"Review #54: The best book I've read this year.\",\n          \"Review #71: Neither good nor bad, just fine.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sentiment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0,\n          2,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Overall Sentiment\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Negative\",\n          \"Positive\",\n          \"Neutral\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Writing Style\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Positive\",\n          \"Decent\",\n          \"Negative\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Emotional Impact\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Positive\",\n          \"Mediocre\",\n          \"Neutral\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Pacing\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Ending\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Response\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 53,\n        \"samples\": [\n          \"Thank you for your positive review! We're glad you enjoyed the book and found the writing style and emotional impact to be captivating. If you have any suggestions for improvement, we would love to hear them.\",\n          \"Thank you for taking the time to share your feedback. Sorry to hear that you found the book to be a waste of time. We appreciate your input and will review it to see how we can improve our selection.\",\n          \"Thank you for your review! We appreciate your feedback and would love to hear more about what you would have liked to see in the book's pacing and ending.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.crosstab(\n",
        "    final_data_4['Writing Style'],\n",
        "    final_data_4['Overall Sentiment'],\n",
        "    normalize='index'\n",
        ").round(2) * 100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "eRpRVm_iQcAN",
        "outputId": "950a7be3-a5a8-49d7-a592-38e6548cc7d6"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Overall Sentiment  Negative  Neutral  Positive\n",
              "Writing Style                                 \n",
              "Decent                  0.0    100.0       0.0\n",
              "Negative              100.0      0.0       0.0\n",
              "Neutral                20.0     80.0       0.0\n",
              "Positive                0.0      0.0     100.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2e2cc38e-78c8-456d-a56e-8b277e1c1566\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>Overall Sentiment</th>\n",
              "      <th>Negative</th>\n",
              "      <th>Neutral</th>\n",
              "      <th>Positive</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Writing Style</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Decent</th>\n",
              "      <td>0.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Negative</th>\n",
              "      <td>100.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Neutral</th>\n",
              "      <td>20.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Positive</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2e2cc38e-78c8-456d-a56e-8b277e1c1566')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2e2cc38e-78c8-456d-a56e-8b277e1c1566 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2e2cc38e-78c8-456d-a56e-8b277e1c1566');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-d4f7c3a7-48f7-4185-a7f4-882c83b44596\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d4f7c3a7-48f7-4185-a7f4-882c83b44596')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-d4f7c3a7-48f7-4185-a7f4-882c83b44596 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \")\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"Writing Style\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Negative\",\n          \"Positive\",\n          \"Decent\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Negative\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 47.60952285695233,\n        \"min\": 0.0,\n        \"max\": 100.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.0,\n          100.0,\n          20.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Neutral\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 52.59911279353167,\n        \"min\": 0.0,\n        \"max\": 100.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          100.0,\n          0.0,\n          80.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Positive\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 50.0,\n        \"min\": 0.0,\n        \"max\": 100.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          100.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Key Findings**:\n",
        "\n",
        "- Reviews that describe the writing style positively are strongly aligned with positive overall sentiment.\n",
        "- Reviews rating the writing style as neutral are mostly linked to neutral or positive overall sentiment, but the positivity rate is noticeably lower than with positive writing style.\n",
        "- Although few reviews explicitly rated writing style as negative, those that did were almost exclusively tied to negative overall sentiment."
      ],
      "metadata": {
        "id": "OPyMwt31P77j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLM-Based Multi Sentiment Analysis Pipeline\n",
        "\n",
        "- Overall Sentiment Detection\n",
        "I started by identifying the overall sentiment of each review using the LLM.\n",
        "\n",
        "- Aspect-Based Sentiment Analysis\n",
        "Next, I extracted sentiment for specific aspects of the book—such as writing style, pacing better understand which elements influenced the reader's opinion.\n",
        "\n",
        "- Liked and Disliked Features\n",
        "I also identified what readers liked or disliked about each aspect, providing more detailed insights (e.g., “engaging writing style,” “rushed ending”).\n",
        "\n",
        "- Automated Customer Response\n",
        "Finally, I generated a personalized response for each review, reflecting the tone and content of the reader's feedback.\n"
      ],
      "metadata": {
        "id": "1ObTyWUVYjCC"
      }
    }
  ]
}